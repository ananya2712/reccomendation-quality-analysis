{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team 20 - ComparingRecommendationQualityUsingLightGCN .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Team Members:\n",
        "\n",
        "\n",
        "1.   Ananya Uppal - PES1UG19CS058 - Section : A\n",
        "2.   Maitreyi P - PES1UG19CS254 - Section : D\n",
        "3.   P Shreya - PES1UG19CS318 - Section : E\n",
        "4.   Trisha Jain - PES1UG19CS542 - Section : I\n",
        "\n",
        "Project Title: A comparitive study of recommendation quality using LightGCN"
      ],
      "metadata": {
        "id": "m3jije_UcAKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Comparative Study of Recommendation Quality Using LightGCN"
      ],
      "metadata": {
        "id": "QEJ7QO15Yu3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries and packages"
      ],
      "metadata": {
        "id": "tPM2yfNSZoVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install tensorly\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "IpkF0bQ1AQ1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import os.path as osp\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "import random\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.max_rows = 10\n",
        "from sklearn import metrics\n",
        "from tensorly import decomposition\n",
        "\n",
        "import torch\n",
        "from torch.functional import tensordot\n",
        "from torch import nn, optim, Tensor\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Dataset, Data, download_url, extract_zip\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.typing import Adj"
      ],
      "metadata": {
        "id": "BZMnkBYc_r2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch has version {torch.__version__}\")\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"Torch geometric version: {torch_geometric.__version__}\")"
      ],
      "metadata": {
        "id": "670tbpEh_3QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the configuration dictionary and preparing the data"
      ],
      "metadata": {
        "id": "tphm4UGVZ2IA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the configuration dictionary "
      ],
      "metadata": {
        "id": "kq3S5se0ftLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_threshold = 3  #@param {type: \"integer\"}: Ratings equal to or greater than 3 are positive items.\n",
        "\n",
        "config_dict = {\n",
        "    \"num_samples_per_user\": 500,\n",
        "    \"num_users\": 200,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 128,\n",
        "    \"lr\": 0.001,\n",
        "    \"weight_decay\": 0.1,\n",
        "    \"embedding_size\": 64,\n",
        "    \"num_layers\": 5,\n",
        "    \"K\": 10,\n",
        "    \"mf_rank\": 8,\n",
        "    \"minibatch_per_print\": 100,\n",
        "    \"epochs_per_print\": 1,\n",
        "    \"val_frac\": 0.2,\n",
        "    \"test_frac\": 0.1,\n",
        "\n",
        "    \"model_name\": \"model.pth\"\n",
        "}"
      ],
      "metadata": {
        "id": "39dGEe9PBog3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\""
      ],
      "metadata": {
        "id": "HekPpNNkBs8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting the dataset into torch geometric format"
      ],
      "metadata": {
        "id": "b4rcqcAkaKXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trans_ml(dat, thres):\n",
        "    \"\"\"\n",
        "    Transform function that assign non-negative entries >= thres 1, and non-\n",
        "    negative entries <= thres 0. Keep other entries the same.\n",
        "    \"\"\"\n",
        "    thres = thres[0]\n",
        "    matrix = dat['edge_index']\n",
        "    matrix[(matrix < thres) & (matrix > -1)] = 0\n",
        "    matrix[(matrix >= thres)] = 1\n",
        "    dat['edge_index'] = matrix\n",
        "    return dat\n",
        "\n",
        "\n",
        "class MovieLens(Dataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None,\n",
        "            transform_args=None, pre_transform_args=None):\n",
        "        \"\"\"\n",
        "        root = where the dataset should be stored. This folder is split\n",
        "        into raw_dir (downloaded dataset) and processed_dir (process data).\n",
        "        \"\"\"\n",
        "        super(MovieLens, self).__init__(root, transform, pre_transform)\n",
        "        self.transform = transform\n",
        "        self.pre_transform = pre_transform\n",
        "        self.transform_args = transform_args\n",
        "        self.pre_transform_args = pre_transform_args\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return \"ml-1m.zip\"\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [\"data_movielens.pt\"]\n",
        "\n",
        "    def download(self):\n",
        "        # Download to `self.raw_dir`.\n",
        "        download_url(DATA_PATH, self.raw_dir)\n",
        "\n",
        "    def _load(self):\n",
        "        print(self.raw_dir)\n",
        "        # extract_zip(self.raw_paths[0], self.raw_dir)\n",
        "        with zipfile.ZipFile(self.raw_paths[0], 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.raw_dir)\n",
        "        unames = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
        "        users = pd.read_table(self.raw_dir+'/ml-1m/users.dat', sep='::', header=None, names=unames,engine='python', encoding='latin-1')\n",
        "\n",
        "        rnames = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
        "        ratings = pd.read_table(self.raw_dir+'/ml-1m/ratings.dat', sep='::', header=None, names=rnames, engine='python',encoding='latin-1')\n",
        "\n",
        "        mnames = ['movie_id', 'title', 'genres']\n",
        "        movies = pd.read_table(self.raw_dir+'/ml-1m/movies.dat', sep='::', header=None, names=mnames, engine='python', encoding='latin-1')\n",
        "\n",
        "        dat = pd.merge(pd.merge(ratings, users), movies)\n",
        "\n",
        "        return users, ratings, movies, dat\n",
        "\n",
        "    def process(self):\n",
        "        print('run process')\n",
        "        # load information from file\n",
        "        users, ratings, movies, dat = self._load()\n",
        "\n",
        "        users = users['user_id']\n",
        "        movies = movies['movie_id']\n",
        "\n",
        "        num_users = config_dict[\"num_users\"]\n",
        "        if num_users != -1:\n",
        "            users = users[:num_users]\n",
        "\n",
        "        user_ids = range(len(users))\n",
        "        movie_ids = range(len(movies))\n",
        "\n",
        "        user_to_id = dict(zip(users, user_ids))\n",
        "        movie_to_id = dict(zip(movies, movie_ids))\n",
        "\n",
        "        # get adjacency info\n",
        "        self.num_user = users.shape[0]\n",
        "        self.num_item = movies.shape[0]\n",
        "\n",
        "        # initialize the adjacency matrix\n",
        "        rat = torch.zeros(self.num_user, self.num_item)\n",
        "\n",
        "        for index, row in ratings.iterrows():\n",
        "            user, movie, rating = row[:3]\n",
        "            if num_users != -1:\n",
        "                if user not in user_to_id: break\n",
        "            # create ratings matrix where (i, j) entry represents the ratings of movie j given by user i.\n",
        "            rat[user_to_id[user], movie_to_id[movie]] = rating\n",
        "\n",
        "        # create Data object\n",
        "        data = Data(edge_index = rat, raw_edge_index = rat.clone(), data = ratings,users = users, items = movies)\n",
        "\n",
        "        # apply any pre-transformation\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data, self.pre_transform_args)\n",
        "\n",
        "        # apply any post_transformation\n",
        "        # if self.transform is not None:\n",
        "        #     # data = self.transform(data, self.transform_args)\n",
        "        data = self.transform(data, [rating_threshold])\n",
        "\n",
        "        # save the processed data into .pt file\n",
        "        torch.save(data, osp.join(self.processed_dir, f'data_movielens.pt'))\n",
        "        print('process finished')\n",
        "      \n",
        "    def len(self):\n",
        "       #returns the number of examples in the graph\n",
        "        return \n",
        "\n",
        "    def get(self):\n",
        "        #load a single graph\n",
        "        data = torch.load(osp.join(self.processed_dir, 'data_movielens.pt'))\n",
        "        return data\n",
        "\n",
        "    def train_val_test_split(self, val_frac=0.2, test_frac=0.1):\n",
        "        #returns two mask matrices (M, N) that represents edges present in the train and validation set\n",
        "        try:\n",
        "            self.num_user, self.num_item\n",
        "        except AttributeError:\n",
        "            data = self.get()\n",
        "            self.num_user = len(data[\"users\"].unique())\n",
        "            self.num_item = len(data[\"items\"].unique())\n",
        "        # get number of edges masked for training and validation\n",
        "        num_train_replaced = \\\n",
        "            round((test_frac+val_frac)*self.num_user*self.num_item)\n",
        "        num_val_show = round(val_frac*self.num_user*self.num_item)\n",
        "\n",
        "        # edges masked during training\n",
        "        indices_user = np.random.randint(0, self.num_user, num_train_replaced)\n",
        "        indices_item = np.random.randint(0, self.num_item, num_train_replaced)\n",
        "        \n",
        "        # sample part of edges from training stage to be unmasked during\n",
        "        # validation\n",
        "        indices_val_user = np.random.choice(indices_user, num_val_show)\n",
        "        indices_val_item = np.random.choice(indices_item, num_val_show)\n",
        "\n",
        "        train_mask = torch.ones(self.num_user, self.num_item)\n",
        "        train_mask[indices_user, indices_item] = 0\n",
        "\n",
        "        val_mask = train_mask.clone()\n",
        "        val_mask[indices_val_user, indices_val_item] = 1\n",
        "\n",
        "        test_mask = torch.ones_like(train_mask)\n",
        "\n",
        "        return train_mask, val_mask, test_mask"
      ],
      "metadata": {
        "id": "egGdCgA5Bv_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Designing the GCN Layer using different techniques"
      ],
      "metadata": {
        "id": "_-guDETJfblw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Designing the GCN layer using the neighboring user counts as weight for aggregation"
      ],
      "metadata": {
        "id": "poEoxwXCaXu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCNConv(MessagePassing):\n",
        "    r\"\"\"The neighbor aggregation operator from the `\"LightGCN: Simplifying and\n",
        "    Powering Graph Convolution Network for Recommendation\"\n",
        "    <https://arxiv.org/abs/2002.02126#>`_ paper\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
        "            the size from the first input(s) to the forward method.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_users (int): Number of users for recommendation.\n",
        "        num_items (int): Number of items to recommend.\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 num_users: int, num_items: int, **kwargs):\n",
        "        super(LightGCNConv, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        pass  # There are no layer parameters to learn.\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj) -> Tensor:\n",
        "        \"\"\"Performs neighborhood aggregation for user/item embeddings.\"\"\"\n",
        "        user_item = torch.zeros(self.num_users, self.num_items, device=x.device)\n",
        "        user_item[edge_index[:, 0], edge_index[:, 1]] = 1\n",
        "        user_neighbor_counts = torch.sum(user_item, axis=1)\n",
        "        item_neightbor_counts = torch.sum(user_item, axis=0)\n",
        "\n",
        "        item_neighbor_matrix = item_neightbor_counts.repeat(self.num_users, 1)\n",
        "        # Compute weight for aggregation: 1 / (N_u)\n",
        "        weights = user_item / (\n",
        "                user_neighbor_counts.repeat(self.num_items, 1).T \\\n",
        "                * torch.ones(item_neighbor_matrix.shape))\n",
        "        weights = torch.nan_to_num(weights, nan=0)\n",
        "        out = torch.concat((weights.T @ x[:self.num_users],\n",
        "                            weights @ x[self.num_users:]), 0)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)"
      ],
      "metadata": {
        "id": "nkyUrzzkBzfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Designing the GCN layer using the neighboring item counts as weight for aggregation"
      ],
      "metadata": {
        "id": "5JrXF7jRavYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCNConv2(MessagePassing):\n",
        "    r\"\"\"The neighbor aggregation operator from the `\"LightGCN: Simplifying and\n",
        "    Powering Graph Convolution Network for Recommendation\"\n",
        "    <https://arxiv.org/abs/2002.02126#>`_ paper\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
        "            the size from the first input(s) to the forward method.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_users (int): Number of users for recommendation.\n",
        "        num_items (int): Number of items to recommend.\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 num_users: int, num_items: int, **kwargs):\n",
        "        super(LightGCNConv2, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        pass  # There are no layer parameters to learn.\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj) -> Tensor:\n",
        "        \"\"\"Performs neighborhood aggregation for user/item embeddings.\"\"\"\n",
        "        user_item = \\\n",
        "                torch.zeros(self.num_users, self.num_items, device=x.device)\n",
        "        user_item[edge_index[:, 0], edge_index[:, 1]] = 1\n",
        "        user_neighbor_counts = torch.sum(user_item, axis=1)\n",
        "        item_neightbor_counts = torch.sum(user_item, axis=0)\n",
        "\n",
        "        user_neighbor_matrix = user_neighbor_counts.repeat(self.num_items, 1)\n",
        "        # Compute weight for aggregation: 1 / (N_i)\n",
        "        weights = user_item / (\n",
        "                torch.ones(user_neighbor_matrix.shape).T \\\n",
        "                * item_neightbor_counts.repeat(self.num_users, 1))\n",
        "        weights = torch.nan_to_num(weights, nan=0)\n",
        "        out = torch.concat((weights.T @ x[:self.num_users],\n",
        "                            weights @ x[self.num_users:]), 0)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)"
      ],
      "metadata": {
        "id": "rDWCIbWiKzM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Designing the GCN layer using the neighboring user counts as well as neighboring item counts as weight for aggregation"
      ],
      "metadata": {
        "id": "PYZk-9vaazQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCNConv3(MessagePassing):\n",
        "    r\"\"\"The neighbor aggregation operator from the `\"LightGCN: Simplifying and\n",
        "    Powering Graph Convolution Network for Recommendation\"\n",
        "    <https://arxiv.org/abs/2002.02126#>`_ paper\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
        "            the size from the first input(s) to the forward method.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_users (int): Number of users for recommendation.\n",
        "        num_items (int): Number of items to recommend.\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 num_users: int, num_items: int, **kwargs):\n",
        "        super(LightGCNConv3, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        pass  # There are no layer parameters to learn.\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj) -> Tensor:\n",
        "        \"\"\"Performs neighborhood aggregation for user/item embeddings.\"\"\"\n",
        "        user_item = \\\n",
        "                torch.zeros(self.num_users, self.num_items, device=x.device)\n",
        "        user_item[edge_index[:, 0], edge_index[:, 1]] = 1\n",
        "        user_neighbor_counts = torch.sum(user_item, axis=1)\n",
        "        item_neightbor_counts = torch.sum(user_item, axis=0)\n",
        "\n",
        "        user_neighbor_matrix = user_neighbor_counts.repeat(self.num_items, 1)\n",
        "        # Compute weight for aggregation: 1 / (N_u * N_i)\n",
        "        weights = user_item / (\n",
        "                user_neighbor_counts.repeat(self.num_items, 1).T \\\n",
        "                * item_neightbor_counts.repeat(self.num_users, 1))\n",
        "        weights = torch.nan_to_num(weights, nan=0)\n",
        "        out = torch.concat((weights.T @ x[:self.num_users],\n",
        "                            weights @ x[self.num_users:]), 0)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)"
      ],
      "metadata": {
        "id": "SxpKuu56PHZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Designing the GCN layer without weights for aggregation"
      ],
      "metadata": {
        "id": "jRqg1s0ka8nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCNConv4(MessagePassing):\n",
        "    r\"\"\"The neighbor aggregation operator from the `\"LightGCN: Simplifying and\n",
        "    Powering Graph Convolution Network for Recommendation\"\n",
        "    <https://arxiv.org/abs/2002.02126#>`_ paper\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
        "            the size from the first input(s) to the forward method.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_users (int): Number of users for recommendation.\n",
        "        num_items (int): Number of items to recommend.\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 num_users: int, num_items: int, **kwargs):\n",
        "        super(LightGCNConv4, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        pass  # There are no layer parameters to learn.\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj) -> Tensor:\n",
        "        \"\"\"Performs neighborhood aggregation for user/item embeddings.\"\"\"\n",
        "        user_item = \\\n",
        "                torch.zeros(self.num_users, self.num_items, device=x.device)\n",
        "        user_item[edge_index[:, 0], edge_index[:, 1]] = 1\n",
        "        user_neighbor_counts = torch.sum(user_item, axis=1)\n",
        "        item_neightbor_counts = torch.sum(user_item, axis=0)\n",
        "\n",
        "        user_neighbor_matrix = user_neighbor_counts.repeat(self.num_items, 1)\n",
        "        item_neighbor_matrix = item_neightbor_counts.repeat(self.num_users, 1)\n",
        "        # Compute weight for aggregation: 1 / (N_u * N_i)\n",
        "        weights = user_item / (\n",
        "                torch.ones(user_neighbor_matrix.shape).T \\\n",
        "                * torch.ones(item_neighbor_matrix.shape))\n",
        "        weights = torch.nan_to_num(weights, nan=0)\n",
        "        out = torch.concat((weights.T @ x[:self.num_users],\n",
        "                            weights @ x[self.num_users:]), 0)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)"
      ],
      "metadata": {
        "id": "7k_biX-EIx35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the models using different techniques"
      ],
      "metadata": {
        "id": "rFbhd4HafLA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Technique 1 - Using neighboring user counts \n",
        "2.   Technique 2 - Using neighboring item counts\n",
        "3.   Technique 3 - Using both neighboring item and neighboring user counts\n",
        "4.   Technique 4 - Using no weights for aggregation\n"
      ],
      "metadata": {
        "id": "N8W3SwNpgZuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building LightGCN model using technique 1"
      ],
      "metadata": {
        "id": "cOo-QaC8bvA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 config: dict,\n",
        "                 device=None,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_users  = config[\"n_users\"]\n",
        "        self.num_items  = config[\"m_items\"]\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "        self.in_channels = self.embedding_size\n",
        "        self.out_channels = self.embedding_size\n",
        "        self.num_layers = config[\"num_layers\"]\n",
        "\n",
        "        # 0-th layer embedding.\n",
        "        self.embedding_user_item = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_users + self.num_items,\n",
        "            embedding_dim=self.embedding_size)\n",
        "        self.alpha = None\n",
        "\n",
        "        # random normal init seems to be a better choice when lightGCN actually\n",
        "        # don't use any non-linear activation function\n",
        "        nn.init.normal_(self.embedding_user_item.weight, std=0.1)\n",
        "        print('use NORMAL distribution initilizer')\n",
        "\n",
        "        self.f = nn.Sigmoid()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(LightGCNConv(\n",
        "                self.embedding_size, self.embedding_size,\n",
        "                num_users=self.num_users, num_items=self.num_items, **kwargs))\n",
        "\n",
        "        for _ in range(1, self.num_layers):\n",
        "            self.convs.append(\n",
        "                LightGCNConv(\n",
        "                        self.embedding_size, self.embedding_size, \n",
        "                        num_users=self.num_users, num_items=self.num_items,\n",
        "                        **kwargs))\n",
        "\n",
        "        self.device = None\n",
        "        if device is not None:\n",
        "            self.convs.to(device)\n",
        "            self.device = device\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n",
        "        xs: List[Tensor] = []\n",
        "\n",
        "        edge_index = torch.nonzero(edge_index)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index, *args, **kwargs)\n",
        "            if self.device is not None:\n",
        "                x = x.to(self.device)\n",
        "            xs.append(x)\n",
        "        xs = torch.stack(xs)\n",
        "        \n",
        "        self.alpha = 1 / (1 + self.num_layers) * torch.ones(xs.shape)\n",
        "        if self.device is not None:\n",
        "            self.alpha = self.alpha.to(self.device)\n",
        "            xs = xs.to(self.device)\n",
        "        x = (xs * self.alpha).sum(dim=0)  # Sum along K layers.\n",
        "        return x\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, num_layers={self.num_layers})')"
      ],
      "metadata": {
        "id": "lQAPcDjWB2pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building LightGCN model using technique 2"
      ],
      "metadata": {
        "id": "Mf6JGY40b672"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCN2(nn.Module):\n",
        "    def __init__(self, \n",
        "                 config: dict,\n",
        "                 device=None,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_users  = config[\"n_users\"]\n",
        "        self.num_items  = config[\"m_items\"]\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "        self.in_channels = self.embedding_size\n",
        "        self.out_channels = self.embedding_size\n",
        "        self.num_layers = config[\"num_layers\"]\n",
        "\n",
        "        # 0-th layer embedding.\n",
        "        self.embedding_user_item = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_users + self.num_items,\n",
        "            embedding_dim=self.embedding_size)\n",
        "        self.alpha = None\n",
        "\n",
        "        # random normal init seems to be a better choice when lightGCN actually\n",
        "        # don't use any non-linear activation function\n",
        "        nn.init.normal_(self.embedding_user_item.weight, std=0.1)\n",
        "        print('use NORMAL distribution initilizer')\n",
        "\n",
        "        self.f = nn.Sigmoid()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(LightGCNConv2(\n",
        "                self.embedding_size, self.embedding_size,\n",
        "                num_users=self.num_users, num_items=self.num_items, **kwargs))\n",
        "\n",
        "        for _ in range(1, self.num_layers):\n",
        "            self.convs.append(\n",
        "                LightGCNConv2(\n",
        "                        self.embedding_size, self.embedding_size, \n",
        "                        num_users=self.num_users, num_items=self.num_items,\n",
        "                        **kwargs))\n",
        "\n",
        "        self.device = None\n",
        "        if device is not None:\n",
        "            self.convs.to(device)\n",
        "            self.device = device\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n",
        "        xs: List[Tensor] = []\n",
        "\n",
        "        edge_index = torch.nonzero(edge_index)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index, *args, **kwargs)\n",
        "            if self.device is not None:\n",
        "                x = x.to(self.device)\n",
        "            xs.append(x)\n",
        "        xs = torch.stack(xs)\n",
        "        \n",
        "        self.alpha = 1 / (1 + self.num_layers) * torch.ones(xs.shape)\n",
        "        if self.device is not None:\n",
        "            self.alpha = self.alpha.to(self.device)\n",
        "            xs = xs.to(self.device)\n",
        "        x = (xs * self.alpha).sum(dim=0)  # Sum along K layers.\n",
        "        return x\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, num_layers={self.num_layers})')"
      ],
      "metadata": {
        "id": "WJJUkuvEMO2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building LightGCN model using technique 3"
      ],
      "metadata": {
        "id": "ECceNd_ub8tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCN3(nn.Module):\n",
        "    def __init__(self, \n",
        "                 config: dict,\n",
        "                 device=None,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_users  = config[\"n_users\"]\n",
        "        self.num_items  = config[\"m_items\"]\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "        self.in_channels = self.embedding_size\n",
        "        self.out_channels = self.embedding_size\n",
        "        self.num_layers = config[\"num_layers\"]\n",
        "\n",
        "        # 0-th layer embedding.\n",
        "        self.embedding_user_item = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_users + self.num_items,\n",
        "            embedding_dim=self.embedding_size)\n",
        "        self.alpha = None\n",
        "\n",
        "        # random normal init seems to be a better choice when lightGCN actually\n",
        "        # don't use any non-linear activation function\n",
        "        nn.init.normal_(self.embedding_user_item.weight, std=0.1)\n",
        "        print('use NORMAL distribution initilizer')\n",
        "\n",
        "        self.f = nn.Sigmoid()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(LightGCNConv3(\n",
        "                self.embedding_size, self.embedding_size,\n",
        "                num_users=self.num_users, num_items=self.num_items, **kwargs))\n",
        "\n",
        "        for _ in range(1, self.num_layers):\n",
        "            self.convs.append(\n",
        "                LightGCNConv3(\n",
        "                        self.embedding_size, self.embedding_size, \n",
        "                        num_users=self.num_users, num_items=self.num_items,\n",
        "                        **kwargs))\n",
        "\n",
        "        self.device = None\n",
        "        if device is not None:\n",
        "            self.convs.to(device)\n",
        "            self.device = device\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n",
        "        xs: List[Tensor] = []\n",
        "\n",
        "        edge_index = torch.nonzero(edge_index)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index, *args, **kwargs)\n",
        "            if self.device is not None:\n",
        "                x = x.to(self.device)\n",
        "            xs.append(x)\n",
        "        xs = torch.stack(xs)\n",
        "        \n",
        "        self.alpha = 1 / (1 + self.num_layers) * torch.ones(xs.shape)\n",
        "        if self.device is not None:\n",
        "            self.alpha = self.alpha.to(self.device)\n",
        "            xs = xs.to(self.device)\n",
        "        x = (xs * self.alpha).sum(dim=0)  # Sum along K layers.\n",
        "        return x\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, num_layers={self.num_layers})')"
      ],
      "metadata": {
        "id": "BkxuokH3Pdgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building LightGCN model using technique 4"
      ],
      "metadata": {
        "id": "RB0tYUCVb-YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCN4(nn.Module):\n",
        "    def __init__(self, \n",
        "                 config: dict,\n",
        "                 device=None,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_users  = config[\"n_users\"]\n",
        "        self.num_items  = config[\"m_items\"]\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "        self.in_channels = self.embedding_size\n",
        "        self.out_channels = self.embedding_size\n",
        "        self.num_layers = config[\"num_layers\"]\n",
        "\n",
        "        # 0-th layer embedding.\n",
        "        self.embedding_user_item = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_users + self.num_items,\n",
        "            embedding_dim=self.embedding_size)\n",
        "        self.alpha = None\n",
        "\n",
        "        # random normal init seems to be a better choice when lightGCN actually\n",
        "        # don't use any non-linear activation function\n",
        "        nn.init.normal_(self.embedding_user_item.weight, std=0.1)\n",
        "        print('use NORMAL distribution initilizer')\n",
        "\n",
        "        self.f = nn.Sigmoid()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(LightGCNConv4(\n",
        "                self.embedding_size, self.embedding_size,\n",
        "                num_users=self.num_users, num_items=self.num_items, **kwargs))\n",
        "\n",
        "        for _ in range(1, self.num_layers):\n",
        "            self.convs.append(\n",
        "                LightGCNConv4(\n",
        "                        self.embedding_size, self.embedding_size, \n",
        "                        num_users=self.num_users, num_items=self.num_items,\n",
        "                        **kwargs))\n",
        "\n",
        "        self.device = None\n",
        "        if device is not None:\n",
        "            self.convs.to(device)\n",
        "            self.device = device\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n",
        "        xs: List[Tensor] = []\n",
        "\n",
        "        edge_index = torch.nonzero(edge_index)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index, *args, **kwargs)\n",
        "            if self.device is not None:\n",
        "                x = x.to(self.device)\n",
        "            xs.append(x)\n",
        "        xs = torch.stack(xs)\n",
        "        \n",
        "        self.alpha = 1 / (1 + self.num_layers) * torch.ones(xs.shape)\n",
        "        if self.device is not None:\n",
        "            self.alpha = self.alpha.to(self.device)\n",
        "            xs = xs.to(self.device)\n",
        "        x = (xs * self.alpha).sum(dim=0)  # Sum along K layers.\n",
        "        return x\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, num_layers={self.num_layers})')"
      ],
      "metadata": {
        "id": "AQ0IUV7lJVk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the ratings and embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "N2jb-QskcJYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getUsersRating(model, users, data):\n",
        "    \"\"\" Get the embedding of users\n",
        "    INPUT:\n",
        "        model: the LightGCN model you are training on\n",
        "        users: this is the user index (note: use 0-indexed and not user number,\n",
        "            which is 1-indexed)\n",
        "        data: the entire data, used to fetch all users and all items\n",
        "    \"\"\"\n",
        "    all_users_items = model(model.embedding_user_item.weight.clone(),\n",
        "                            data[\"edge_index\"])\n",
        "    all_users = all_users_items[:len(data[\"users\"])]\n",
        "    items_emb = all_users_items[len(data[\"users\"]):]\n",
        "    users_emb = all_users[users.long()]\n",
        "    rating = model.f(torch.matmul(users_emb, items_emb.t()))\n",
        "    return rating\n",
        "\n",
        "def getEmbedding(model, users, pos, neg, data, mask):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        model: the LightGCN model you are training on\n",
        "        users: this is the user index (note: use 0-indexed and not user number,\n",
        "            which is 1-indexed)\n",
        "        pos: positive index corresponding to an item that the user like\n",
        "        neg: negative index corresponding to an item that the user doesn't like\n",
        "        data: the entire data, used to fetch all users and all items\n",
        "        mask: Masking matrix indicating edges present in the current\n",
        "            train / validation / test set.\n",
        "    \"\"\"\n",
        "    # assuming we always search for users and items by their indices (instead of\n",
        "    # user/item number)\n",
        "    all_users_items = model(model.embedding_user_item.weight.clone(),data[\"edge_index\"] * mask)\n",
        "    all_users = all_users_items[:len(data[\"users\"])]\n",
        "    all_items = all_users_items[len(data[\"users\"]):]\n",
        "    users_emb = all_users[users]\n",
        "    pos_emb = all_items[pos]\n",
        "    neg_emb = all_items[neg]\n",
        "    n_user = len(data[\"users\"])\n",
        "    users_emb_ego = model.embedding_user_item(users)\n",
        "    # offset the index to fetch embedding from user_item\n",
        "    pos_emb_ego = model.embedding_user_item(pos + n_user)\n",
        "    neg_emb_ego = model.embedding_user_item(neg + n_user)\n",
        "    return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego"
      ],
      "metadata": {
        "id": "B6TLkqgvB5Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the evaluation metrics"
      ],
      "metadata": {
        "id": "PgdBgmYXgOMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing the Bayesian Personalised Ranking Loss"
      ],
      "metadata": {
        "id": "MgIpKQSRcbwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(model, users, pos, neg, data, mask):\n",
        "    \"\"\" \n",
        "    INPUT:\n",
        "        model: the LightGCN model you are training on\n",
        "        users: this is the user index (note: use 0-indexed and not user number,\n",
        "            which is 1-indexed)\n",
        "        pos: positive index corresponding to an item that the user like\n",
        "            (0-indexed, note to index items starting from 0)\n",
        "        neg: negative index corresponding to an item that the user doesn't like\n",
        "        data: the entire data, used to fetch all users and all items\n",
        "        mask: Masking matrix indicating edges present in the current\n",
        "            train / validation / test set.\n",
        "    OUTPUT:\n",
        "        loss, reg_loss\n",
        "    \"\"\"\n",
        "    # assuming we always sample the same number of positive and negative sample\n",
        "    # per user\n",
        "    assert len(users) == len(pos) and len(users) == len(neg)\n",
        "    (users_emb, pos_emb, neg_emb, \n",
        "    userEmb0,  posEmb0, negEmb0) = getEmbedding(model, users.long(), pos.long(),\n",
        "                                                neg.long(), data, mask)\n",
        "    reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
        "                        posEmb0.norm(2).pow(2)  +\n",
        "                        negEmb0.norm(2).pow(2))/float(len(users))\n",
        "    pos_scores = torch.mul(users_emb, pos_emb)\n",
        "    pos_scores = torch.sum(pos_scores, dim=1)\n",
        "    neg_scores = torch.mul(users_emb, neg_emb)\n",
        "    neg_scores = torch.sum(neg_scores, dim=1)\n",
        "    \n",
        "    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
        "    \n",
        "    return loss, reg_loss"
      ],
      "metadata": {
        "id": "bU9QwbbOB7mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing the TopK precision and recall "
      ],
      "metadata": {
        "id": "vwB4bGRKcj3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def personalized_topk(pred, K, user_indices, edge_index):\n",
        "    \"\"\"Computes TopK precision and recall.\n",
        "\n",
        "    Args:\n",
        "        pred: Predicted similarities between user and item.\n",
        "        K: Number of items to rank.\n",
        "        user_indices: Indices of users for each prediction in `pred`.\n",
        "        edge_index: User and item connection matrix.\n",
        "\n",
        "    Returns:\n",
        "        Average Top K precision and recall for users in `user_indices`.\n",
        "    \"\"\"\n",
        "    per_user_preds = collections.defaultdict(list)\n",
        "    for index, user in enumerate(user_indices):\n",
        "        per_user_preds[user.item()].append(pred[index].item())\n",
        "    precisions = 0.0\n",
        "    recalls = 0.0\n",
        "    for user, preds in per_user_preds.items():\n",
        "        while len(preds) < K:\n",
        "            preds.append(random.choice(range(edge_index.shape[1])))\n",
        "        top_ratings, top_items = torch.topk(torch.tensor(preds), K)\n",
        "        correct_preds = edge_index[user, top_items].sum().item()\n",
        "        total_pos = edge_index[user].sum().item()\n",
        "        precisions += correct_preds / K\n",
        "        recalls += correct_preds / total_pos if total_pos != 0 else 0\n",
        "    num_users = len(user_indices.unique())\n",
        "    return precisions / num_users, recalls / num_users"
      ],
      "metadata": {
        "id": "KC5fAbRAB99O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Training, Validation and Testing samples"
      ],
      "metadata": {
        "id": "fGnkFOOrcxFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _sample_pos_neg(data, mask, num_samples_per_user):\n",
        "    \"\"\"Samples (user, positive item, negative item) tuples per user.\n",
        "\n",
        "    If a user does not have a postive (negative) item, we choose an item\n",
        "    with unknown liking (an item without raw rating data).\n",
        "\n",
        "    Args:\n",
        "        data: Dataset object containing edge_index and raw ratings matrix.\n",
        "        mask: Masking matrix indicating edges present in the current\n",
        "            train / validation / test set.\n",
        "        num_samples_per_user: Number of samples to generate for each user.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor object of (user, positive item, negative item) samples.\n",
        "    \"\"\"\n",
        "    print(\"=====Starting to sample=====\")\n",
        "    start = time.time()\n",
        "    samples = []\n",
        "    all_items = set(range(len(data[\"items\"])))\n",
        "    for user_index, user in enumerate(data[\"users\"]):\n",
        "        pos_items = set(\n",
        "            torch.nonzero(data[\"edge_index\"][user_index])[:, 0].tolist())\n",
        "        unknown_items = all_items.difference(\n",
        "                set(\n",
        "                    torch.nonzero(\n",
        "                        data[\"raw_edge_index\"][user_index])[:, 0].tolist()))\n",
        "        neg_items = all_items.difference(\n",
        "            set(pos_items)).difference(set(unknown_items))\n",
        "        unmasked_items = set(torch.nonzero(mask[user_index])[:, 0].tolist())\n",
        "        if len(unknown_items.union(pos_items)) == 0 or \\\n",
        "                len(unknown_items.union(neg_items)) == 0:\n",
        "            continue\n",
        "        for _ in range(num_samples_per_user):\n",
        "            if len(pos_items.intersection(unmasked_items)) == 0:\n",
        "                pos_item_index = random.choice(\n",
        "                    list(unknown_items.intersection(unmasked_items)))\n",
        "            else:\n",
        "                pos_item_index = random.choice(\n",
        "                    list(pos_items.intersection(unmasked_items)))\n",
        "            if len(neg_items.intersection(unmasked_items)) == 0:\n",
        "                neg_item_index = random.choice(\n",
        "                    list(unknown_items.intersection(unmasked_items)))\n",
        "            else:\n",
        "                neg_item_index = random.choice(\n",
        "                    list(neg_items.intersection(unmasked_items)))\n",
        "            samples.append((user_index, pos_item_index, neg_item_index))\n",
        "    end = time.time()\n",
        "    print(f\"=====Sampling completed (took {end - start} seconds)=====\")\n",
        "    return torch.tensor(samples, dtype=torch.int32)\n",
        "\n",
        "def sample_pos_neg(data, train_mask, val_mask, test_mask, num_samples_per_user):\n",
        "    \"\"\"Samples (user, positive item, negative item) tuples per user.\n",
        "\n",
        "    If a user does not have a postive (negative) item, we choose an item\n",
        "    with unknown liking (an item without raw rating data).\n",
        "\n",
        "    Args:\n",
        "        data: Dataset object containing edge_index and raw ratings matrix.\n",
        "        train_mask: Masking matrix indicating edges present in train set.\n",
        "        val_mask: Masking matrix indicating edges present in validation set.\n",
        "        test_mask: Masking matrix indicating edges present in test set.\n",
        "        num_samples_per_user: Number of samples to generate for each user.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor object of (user, positive item, negative item) samples for\n",
        "        train, validation and test.\n",
        "    \"\"\"\n",
        "    train_samples = _sample_pos_neg(data, train_mask, num_samples_per_user)\n",
        "    val_samples = _sample_pos_neg(data, val_mask, num_samples_per_user)\n",
        "    test_samples = _sample_pos_neg(data, test_mask, num_samples_per_user)\n",
        "    return train_samples, val_samples, test_samples"
      ],
      "metadata": {
        "id": "LkMOCdU1CAHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the models"
      ],
      "metadata": {
        "id": "2acWSA-bdPX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model built using Technique 1"
      ],
      "metadata": {
        "id": "_ecvkgN0c7An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.getcwd()\n",
        "movielens = MovieLens(root=root, transform=trans_ml)\n",
        "data = movielens.get()\n",
        "train_mask, val_mask, test_mask = \\\n",
        "        movielens.train_val_test_split(val_frac=config_dict[\"val_frac\"],\n",
        "                                       test_frac=config_dict[\"test_frac\"])\n",
        "\n",
        "n_users = len(data[\"users\"].unique())\n",
        "m_items = len(data[\"items\"].unique())\n",
        "print(f\"#Users: {n_users}\")\n",
        "print(f\"#Items: {m_items}\")\n",
        "\n",
        "model_config = {\n",
        "    \"n_users\": n_users,\n",
        "    \"m_items\": m_items,\n",
        "    \"embedding_size\": config_dict[\"embedding_size\"],\n",
        "    \"num_layers\": config_dict[\"num_layers\"],\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lightGCN = LightGCN(model_config, device=device)\n",
        "\n",
        "num_samples_per_user = config_dict[\"num_samples_per_user\"]\n",
        "epochs = config_dict[\"epochs\"]\n",
        "batch_size = config_dict[\"batch_size\"]\n",
        "lr = config_dict[\"lr\"]\n",
        "weight_decay = config_dict[\"weight_decay\"]\n",
        "\n",
        "K = config_dict[\"K\"]\n",
        "\n",
        "lightGCN.to(device)\n",
        "\n",
        "samples_train, samples_val, samples_test = \\\n",
        "        sample_pos_neg(data, train_mask, val_mask, test_mask,\n",
        "                       num_samples_per_user)\n",
        "\n",
        "samples_train=samples_train.to(device)\n",
        "samples_val=samples_val.to(device)\n",
        "samples_test=samples_test.to(device)\n",
        "train_mask=train_mask.to(device)\n",
        "val_mask=val_mask.to(device)\n",
        "test_mask=test_mask.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "print(f\"#Training samples: {len(samples_train)}\",\n",
        "      f\"#Validation samples: {len(samples_val)}\",\n",
        "      f\"#Test samples: {len(samples_test)}\")\n",
        "\n",
        "optimizer = optim.Adam(lightGCN.parameters(), lr=lr)\n",
        "print(\"Optimizer:\", optimizer)\n",
        "\n",
        "epochs_tracked = []\n",
        "train_topks = []\n",
        "val_topks = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Training on the {} epoch\".format(epoch))\n",
        "    lightGCN.train()\n",
        "    loss_sum = 0\n",
        "    # Shuffle the order of rows.\n",
        "    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n",
        "    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        current_batch = \\\n",
        "            samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
        "        # Shuffle the order of rows.\n",
        "        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n",
        "        users = current_batch[:, 0:1]\n",
        "        pos = current_batch[:, 1:2]\n",
        "        neg = current_batch[:, 2:3]\n",
        "\n",
        "        loss, reg_loss = bpr_loss(lightGCN, users, pos, neg, data,\n",
        "                                  train_mask)\n",
        "        reg_loss = reg_loss * weight_decay\n",
        "        loss = loss + reg_loss\n",
        "        loss_sum += loss.detach()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % config_dict[\"minibatch_per_print\"] == 0:\n",
        "            all_users = torch.linspace(start=0,\n",
        "                                       end=n_users - 1, steps=n_users).long()\n",
        "            user_indices = current_batch[:, 0]\n",
        "            user_indices = user_indices.repeat(2).long()\n",
        "            item_indices = torch.cat(\n",
        "                (current_batch[:, 1], current_batch[:, 2])).long()\n",
        "            pred = getUsersRating(lightGCN,\n",
        "                                  all_users,\n",
        "                                  data)[user_indices, item_indices]\n",
        "            truth = data[\"edge_index\"][user_indices, item_indices]\n",
        "            topk_precision, topk_recall = \\\n",
        "                personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "\n",
        "            print(\"Training on epoch {} minibatch {}/{} completed\\n\".format(epoch, batch_idx+1,\n",
        "                                                                            math.ceil(len(samples_train) / batch_size)),\n",
        "                  \"bpr_loss on current minibatch is {}, and regularization loss is {}.\\n\".format(round(float(loss.detach().cpu()), 6),\n",
        "                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n",
        "                  \"Top K precision = {}, recall = {}.\".format(topk_precision, topk_recall))\n",
        "\n",
        "    if epoch % config_dict[\"epochs_per_print\"] == 0:\n",
        "        epochs_tracked.append(epoch)\n",
        "\n",
        "        # evaluation on both the trainisng and validation set\n",
        "        lightGCN.eval()\n",
        "        # predict on the training set\n",
        "        users = samples_train[:, 0:1]\n",
        "        user_indices = samples_train[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat(\n",
        "            (samples_train[:, 1], samples_train[:, 2])).long()\n",
        "        pred = getUsersRating(lightGCN,\n",
        "                              users[:,0],\n",
        "                              data)[user_indices, item_indices]\n",
        "        truth = data[\"edge_index\"][users.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        train_topk_precision, train_topk_recall = \\\n",
        "            personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "        train_topks.append((train_topk_precision, train_topk_recall))\n",
        "\n",
        "        # predict on the validation set\n",
        "        users_val = samples_val[:, 0:1]\n",
        "        pos_val = samples_val[:, 1:2]\n",
        "        neg_val = samples_val[:, 2:3]\n",
        "\n",
        "        loss_val, reg_loss_val = bpr_loss(\n",
        "            lightGCN, users_val, pos_val, neg_val, data, val_mask)\n",
        "        reg_loss_val = reg_loss_val * weight_decay\n",
        "\n",
        "        # predict on the validation set\n",
        "        user_indices = samples_val[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n",
        "        pred_val = getUsersRating(lightGCN,\n",
        "                                  users_val[:,0],\n",
        "                                  data)[user_indices, item_indices]\n",
        "        truth_val = data[\"edge_index\"][users_val.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        val_topk_precision, val_topk_recall = \\\n",
        "            personalized_topk(pred_val, K, user_indices, data[\"edge_index\"])\n",
        "        val_topks.append((val_topk_precision, val_topk_recall))\n",
        "\n",
        "        print(\"\\nTraining on {} epoch completed.\\n\".format(epoch),\n",
        "              \"Average bpr_loss on train set is {} for the current epoch.\\n\".format(round(float(loss_sum/len(samples_train)), 6)),\n",
        "              \"Training top K precision = {}, recall = {}.\\n\".format(train_topk_precision, train_topk_recall),\n",
        "              \"Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n\".format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n",
        "                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n",
        "              \"Validation top K precision = {}, recall = {}.\\n\".format(val_topk_precision, val_topk_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVJQ-WdkCC_f",
        "outputId": "8dc3c58f-779b-4481-e18d-443f7f8ec467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Users: 200\n",
            "#Items: 3883\n",
            "use NORMAL distribution initilizer\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.7846908569335938 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.6648406982421875 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.7277708053588867 seconds)=====\n",
            "#Training samples: 100000 #Validation samples: 100000 #Test samples: 100000\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Training on the 0 epoch\n",
            "Training on epoch 0 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.789014, and regularization loss is 0.095867.\n",
            " Top K precision = 0.09340659340659337, recall = 0.0069108403837493965.\n",
            "Training on epoch 0 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.738261, and regularization loss is 0.045114.\n",
            " Top K precision = 0.0863157894736842, recall = 0.006725110258641416.\n",
            "Training on epoch 0 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.716785, and regularization loss is 0.023637.\n",
            " Top K precision = 0.08125, recall = 0.008648378831613722.\n",
            "Training on epoch 0 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.706553, and regularization loss is 0.013405.\n",
            " Top K precision = 0.07524752475247522, recall = 0.007910120302280913.\n",
            "Training on epoch 0 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.702525, and regularization loss is 0.009378.\n",
            " Top K precision = 0.09139784946236555, recall = 0.009429417446922026.\n",
            "Training on epoch 0 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.699382, and regularization loss is 0.006235.\n",
            " Top K precision = 0.09494949494949492, recall = 0.00660308782660415.\n",
            "Training on epoch 0 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.697297, and regularization loss is 0.00415.\n",
            " Top K precision = 0.09374999999999999, recall = 0.0077489179456526995.\n",
            "Training on epoch 0 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.695636, and regularization loss is 0.002489.\n",
            " Top K precision = 0.08526315789473682, recall = 0.0057912150199814424.\n",
            "\n",
            "Training on 0 epoch completed.\n",
            " Average bpr_loss on train set is 0.005569 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.007497043732413047.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.06949999999999992, recall = 0.0059811494368266415.\n",
            "\n",
            "Training on the 1 epoch\n",
            "Training on epoch 1 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69613, and regularization loss is 0.002983.\n",
            " Top K precision = 0.07586206896551724, recall = 0.007500285202138237.\n",
            "Training on epoch 1 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.695069, and regularization loss is 0.001922.\n",
            " Top K precision = 0.08041237113402058, recall = 0.007728332396361367.\n",
            "Training on epoch 1 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.694002, and regularization loss is 0.000855.\n",
            " Top K precision = 0.08125, recall = 0.006505175246434643.\n",
            "Training on epoch 1 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.694445, and regularization loss is 0.001298.\n",
            " Top K precision = 0.08586956521739127, recall = 0.007395106355503547.\n",
            "Training on epoch 1 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.694106, and regularization loss is 0.000958.\n",
            " Top K precision = 0.08673469387755098, recall = 0.006376726869108395.\n",
            "Training on epoch 1 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693699, and regularization loss is 0.000552.\n",
            " Top K precision = 0.07263157894736842, recall = 0.006167416770123882.\n",
            "Training on epoch 1 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693659, and regularization loss is 0.000512.\n",
            " Top K precision = 0.08333333333333329, recall = 0.00533162660288193.\n",
            "Training on epoch 1 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693839, and regularization loss is 0.000692.\n",
            " Top K precision = 0.08118811881188115, recall = 0.00717247246333226.\n",
            "\n",
            "Training on 1 epoch completed.\n",
            " Average bpr_loss on train set is 0.00543 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 2 epoch\n",
            "Training on epoch 2 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693474, and regularization loss is 0.000326.\n",
            " Top K precision = 0.1010416666666666, recall = 0.007558929507791454.\n",
            "Training on epoch 2 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693573, and regularization loss is 0.000426.\n",
            " Top K precision = 0.09999999999999995, recall = 0.008068766642346636.\n",
            "Training on epoch 2 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693656, and regularization loss is 0.000509.\n",
            " Top K precision = 0.08210526315789467, recall = 0.008016401019703374.\n",
            "Training on epoch 2 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693364, and regularization loss is 0.000217.\n",
            " Top K precision = 0.09247311827956985, recall = 0.008145004461597865.\n",
            "Training on epoch 2 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693602, and regularization loss is 0.000455.\n",
            " Top K precision = 0.09199999999999997, recall = 0.008605602359920835.\n",
            "Training on epoch 2 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69387, and regularization loss is 0.000723.\n",
            " Top K precision = 0.08199999999999998, recall = 0.0077621255593166546.\n",
            "Training on epoch 2 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693401, and regularization loss is 0.000254.\n",
            " Top K precision = 0.09999999999999998, recall = 0.008896098675188585.\n",
            "Training on epoch 2 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693556, and regularization loss is 0.000409.\n",
            " Top K precision = 0.09285714285714283, recall = 0.008001143334017902.\n",
            "\n",
            "Training on 2 epoch completed.\n",
            " Average bpr_loss on train set is 0.005423 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 3 epoch\n",
            "Training on epoch 3 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693344, and regularization loss is 0.000197.\n",
            " Top K precision = 0.08854166666666664, recall = 0.007874872526177823.\n",
            "Training on epoch 3 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693512, and regularization loss is 0.000365.\n",
            " Top K precision = 0.07692307692307691, recall = 0.006852593933317592.\n",
            "Training on epoch 3 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693341, and regularization loss is 0.000193.\n",
            " Top K precision = 0.08152173913043476, recall = 0.0060386599853586445.\n",
            "Training on epoch 3 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693329, and regularization loss is 0.000181.\n",
            " Top K precision = 0.10212765957446804, recall = 0.008884401492897076.\n",
            "Training on epoch 3 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693553, and regularization loss is 0.000406.\n",
            " Top K precision = 0.08556701030927835, recall = 0.006736327553591974.\n",
            "Training on epoch 3 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693544, and regularization loss is 0.000396.\n",
            " Top K precision = 0.09473684210526313, recall = 0.009626959567676053.\n",
            "Training on epoch 3 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693205, and regularization loss is 5.7e-05.\n",
            " Top K precision = 0.07525773195876286, recall = 0.0062861812417701855.\n",
            "Training on epoch 3 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693248, and regularization loss is 0.000101.\n",
            " Top K precision = 0.0797752808988764, recall = 0.006366720957283189.\n",
            "\n",
            "Training on 3 epoch completed.\n",
            " Average bpr_loss on train set is 0.005422 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.00755877212747478.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 4 epoch\n",
            "Training on epoch 4 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000116.\n",
            " Top K precision = 0.09569892473118276, recall = 0.008628102232050986.\n",
            "Training on epoch 4 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693337, and regularization loss is 0.00019.\n",
            " Top K precision = 0.08282828282828283, recall = 0.0062219110394345035.\n",
            "Training on epoch 4 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69321, and regularization loss is 6.2e-05.\n",
            " Top K precision = 0.06516853932584271, recall = 0.00618767244406441.\n",
            "Training on epoch 4 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693235, and regularization loss is 8.8e-05.\n",
            " Top K precision = 0.08478260869565216, recall = 0.00884673599860669.\n",
            "Training on epoch 4 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693224, and regularization loss is 7.7e-05.\n",
            " Top K precision = 0.10208333333333326, recall = 0.007430329679489855.\n",
            "Training on epoch 4 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.10549450549450545, recall = 0.008281856198656037.\n",
            "Training on epoch 4 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.0891304347826087, recall = 0.006994282634824866.\n",
            "Training on epoch 4 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693255, and regularization loss is 0.000107.\n",
            " Top K precision = 0.08709677419354836, recall = 0.007943120935642683.\n",
            "\n",
            "Training on 4 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474775.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 5 epoch\n",
            "Training on epoch 5 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693201, and regularization loss is 5.4e-05.\n",
            " Top K precision = 0.07448979591836734, recall = 0.0075062316044645205.\n",
            "Training on epoch 5 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693149, and regularization loss is 2e-06.\n",
            " Top K precision = 0.11145833333333327, recall = 0.008277691514148424.\n",
            "Training on epoch 5 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693246, and regularization loss is 9.9e-05.\n",
            " Top K precision = 0.0857142857142857, recall = 0.006267027234064211.\n",
            "Training on epoch 5 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.09191919191919189, recall = 0.006906284050087886.\n",
            "Training on epoch 5 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693227, and regularization loss is 8e-05.\n",
            " Top K precision = 0.08817204301075264, recall = 0.008688509640714317.\n",
            "Training on epoch 5 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693192, and regularization loss is 4.4e-05.\n",
            " Top K precision = 0.08124999999999996, recall = 0.008735963992210896.\n",
            "Training on epoch 5 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693242, and regularization loss is 9.4e-05.\n",
            " Top K precision = 0.09099999999999998, recall = 0.0071400312331364735.\n",
            "Training on epoch 5 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09009900990099004, recall = 0.006743506161641351.\n",
            "\n",
            "Training on 5 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 6 epoch\n",
            "Training on epoch 6 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.0927083333333333, recall = 0.0067729945531998835.\n",
            "Training on epoch 6 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693152, and regularization loss is 4e-06.\n",
            " Top K precision = 0.07978723404255317, recall = 0.006560760882490867.\n",
            "Training on epoch 6 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09270833333333332, recall = 0.006887750750854807.\n",
            "Training on epoch 6 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693181, and regularization loss is 3.4e-05.\n",
            " Top K precision = 0.08947368421052627, recall = 0.007290091983125738.\n",
            "Training on epoch 6 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69321, and regularization loss is 6.2e-05.\n",
            " Top K precision = 0.11157894736842099, recall = 0.008387999322478329.\n",
            "Training on epoch 6 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693178, and regularization loss is 3.1e-05.\n",
            " Top K precision = 0.08404255319148936, recall = 0.008012448248826978.\n",
            "Training on epoch 6 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08437499999999998, recall = 0.0065688598848364504.\n",
            "Training on epoch 6 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693203, and regularization loss is 5.6e-05.\n",
            " Top K precision = 0.10106382978723406, recall = 0.007324756104656574.\n",
            "\n",
            "Training on 6 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007558772127474779.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 7 epoch\n",
            "Training on epoch 7 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.0804347826086956, recall = 0.0065296134508177565.\n",
            "Training on epoch 7 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.0894230769230769, recall = 0.007524371014610726.\n",
            "Training on epoch 7 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693176, and regularization loss is 2.9e-05.\n",
            " Top K precision = 0.08604651162790694, recall = 0.007020021519083566.\n",
            "Training on epoch 7 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09340659340659337, recall = 0.007654760534449795.\n",
            "Training on epoch 7 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
            " Top K precision = 0.0868131868131868, recall = 0.00827257589098629.\n",
            "Training on epoch 7 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0704081632653061, recall = 0.0070406378490407645.\n",
            "Training on epoch 7 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09670329670329668, recall = 0.007794294885007798.\n",
            "Training on epoch 7 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693187, and regularization loss is 4e-05.\n",
            " Top K precision = 0.07978723404255314, recall = 0.006716923392782041.\n",
            "\n",
            "Training on 7 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999991, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 8 epoch\n",
            "Training on epoch 8 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08124999999999998, recall = 0.007845522313138818.\n",
            "Training on epoch 8 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693182, and regularization loss is 3.4e-05.\n",
            " Top K precision = 0.08666666666666666, recall = 0.009024239800045769.\n",
            "Training on epoch 8 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09354838709677414, recall = 0.008699996288647528.\n",
            "Training on epoch 8 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09468085106382973, recall = 0.009178961966788649.\n",
            "Training on epoch 8 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08842105263157893, recall = 0.0062362835423925515.\n",
            "Training on epoch 8 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08775510204081628, recall = 0.007871264088489937.\n",
            "Training on epoch 8 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08947368421052627, recall = 0.00799559151591045.\n",
            "Training on epoch 8 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693181, and regularization loss is 3.3e-05.\n",
            " Top K precision = 0.10104166666666663, recall = 0.007565981706178858.\n",
            "\n",
            "Training on 8 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 9 epoch\n",
            "Training on epoch 9 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
            " Top K precision = 0.08387096774193543, recall = 0.00801927169775389.\n",
            "Training on epoch 9 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693174, and regularization loss is 2.7e-05.\n",
            " Top K precision = 0.07878787878787877, recall = 0.0058397040510943165.\n",
            "Training on epoch 9 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08876404494382022, recall = 0.007941963891918886.\n",
            "Training on epoch 9 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09438202247191006, recall = 0.008693886487709825.\n",
            "Training on epoch 9 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08865979381443292, recall = 0.007656817154449245.\n",
            "Training on epoch 9 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.10329670329670325, recall = 0.008508676179759282.\n",
            "Training on epoch 9 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07391304347826086, recall = 0.006872373004044865.\n",
            "Training on epoch 9 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.07608695652173911, recall = 0.006558503931467808.\n",
            "\n",
            "Training on 9 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474777.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model built using Technique 2"
      ],
      "metadata": {
        "id": "7tTHj_hndBlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.getcwd()\n",
        "movielens = MovieLens(root=root, transform=trans_ml)\n",
        "data = movielens.get()\n",
        "train_mask, val_mask, test_mask = \\\n",
        "        movielens.train_val_test_split(val_frac=config_dict[\"val_frac\"],\n",
        "                                       test_frac=config_dict[\"test_frac\"])\n",
        "\n",
        "n_users = len(data[\"users\"].unique())\n",
        "m_items = len(data[\"items\"].unique())\n",
        "print(f\"#Users: {n_users}\")\n",
        "print(f\"#Items: {m_items}\")\n",
        "\n",
        "model_config = {\n",
        "    \"n_users\": n_users,\n",
        "    \"m_items\": m_items,\n",
        "    \"embedding_size\": config_dict[\"embedding_size\"],\n",
        "    \"num_layers\": config_dict[\"num_layers\"],\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lightGCN2 = LightGCN2(model_config, device=device)\n",
        "\n",
        "num_samples_per_user = config_dict[\"num_samples_per_user\"]\n",
        "epochs = config_dict[\"epochs\"]\n",
        "batch_size = config_dict[\"batch_size\"]\n",
        "lr = config_dict[\"lr\"]\n",
        "weight_decay = config_dict[\"weight_decay\"]\n",
        "\n",
        "K = config_dict[\"K\"]\n",
        "\n",
        "lightGCN2.to(device)\n",
        "\n",
        "samples_train, samples_val, samples_test = \\\n",
        "        sample_pos_neg(data, train_mask, val_mask, test_mask,\n",
        "                       num_samples_per_user)\n",
        "\n",
        "samples_train=samples_train.to(device)\n",
        "samples_val=samples_val.to(device)\n",
        "samples_test=samples_test.to(device)\n",
        "train_mask=train_mask.to(device)\n",
        "val_mask=val_mask.to(device)\n",
        "test_mask=test_mask.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "print(f\"#Training samples: {len(samples_train)}\",\n",
        "      f\"#Validation samples: {len(samples_val)}\",\n",
        "      f\"#Test samples: {len(samples_test)}\")\n",
        "\n",
        "optimizer = optim.Adam(lightGCN2.parameters(), lr=lr)\n",
        "print(\"Optimizer:\", optimizer)\n",
        "\n",
        "epochs_tracked = []\n",
        "train_topks2 = []\n",
        "val_topks2 = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Training on the {} epoch\".format(epoch))\n",
        "    lightGCN2.train()\n",
        "    loss_sum = 0\n",
        "    # Shuffle the order of rows.\n",
        "    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n",
        "    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        current_batch = \\\n",
        "            samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
        "        # Shuffle the order of rows.\n",
        "        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n",
        "        users = current_batch[:, 0:1]\n",
        "        pos = current_batch[:, 1:2]\n",
        "        neg = current_batch[:, 2:3]\n",
        "\n",
        "        loss, reg_loss = bpr_loss(lightGCN2, users, pos, neg, data,\n",
        "                                  train_mask)\n",
        "        reg_loss = reg_loss * weight_decay\n",
        "        loss = loss + reg_loss\n",
        "        loss_sum += loss.detach()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % config_dict[\"minibatch_per_print\"] == 0:\n",
        "            all_users = torch.linspace(start=0,\n",
        "                                       end=n_users - 1, steps=n_users).long()\n",
        "            user_indices = current_batch[:, 0]\n",
        "            user_indices = user_indices.repeat(2).long()\n",
        "            item_indices = torch.cat(\n",
        "                (current_batch[:, 1], current_batch[:, 2])).long()\n",
        "            pred = getUsersRating(lightGCN2,\n",
        "                                  all_users,\n",
        "                                  data)[user_indices, item_indices]\n",
        "            truth = data[\"edge_index\"][user_indices, item_indices]\n",
        "            topk_precision, topk_recall = \\\n",
        "                personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "\n",
        "            print(\"Training on epoch {} minibatch {}/{} completed\\n\".format(epoch, batch_idx+1,\n",
        "                                                                            math.ceil(len(samples_train) / batch_size)),\n",
        "                  \"bpr_loss on current minibatch is {}, and regularization loss is {}.\\n\".format(round(float(loss.detach().cpu()), 6),\n",
        "                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n",
        "                  \"Top K precision = {}, recall = {}.\".format(topk_precision, topk_recall))\n",
        "\n",
        "    if epoch % config_dict[\"epochs_per_print\"] == 0:\n",
        "        epochs_tracked.append(epoch)\n",
        "\n",
        "        # evaluation on both the trainisng and validation set\n",
        "        lightGCN2.eval()\n",
        "        # predict on the training set\n",
        "        users = samples_train[:, 0:1]\n",
        "        user_indices = samples_train[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat(\n",
        "            (samples_train[:, 1], samples_train[:, 2])).long()\n",
        "        pred = getUsersRating(lightGCN2,\n",
        "                              users[:,0],\n",
        "                              data)[user_indices, item_indices]\n",
        "        truth = data[\"edge_index\"][users.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        train_topk_precision, train_topk_recall = \\\n",
        "            personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "        train_topks2.append((train_topk_precision, train_topk_recall))\n",
        "\n",
        "        # predict on the validation set\n",
        "        users_val = samples_val[:, 0:1]\n",
        "        pos_val = samples_val[:, 1:2]\n",
        "        neg_val = samples_val[:, 2:3]\n",
        "\n",
        "        loss_val, reg_loss_val = bpr_loss(\n",
        "            lightGCN2, users_val, pos_val, neg_val, data, val_mask)\n",
        "        reg_loss_val = reg_loss_val * weight_decay\n",
        "\n",
        "        # predict on the validation set\n",
        "        user_indices = samples_val[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n",
        "        pred_val = getUsersRating(lightGCN2,\n",
        "                                  users_val[:,0],\n",
        "                                  data)[user_indices, item_indices]\n",
        "        truth_val = data[\"edge_index\"][users_val.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        val_topk_precision, val_topk_recall = \\\n",
        "            personalized_topk(pred_val, K, user_indices, data[\"edge_index\"])\n",
        "        val_topks2.append((val_topk_precision, val_topk_recall))\n",
        "\n",
        "        print(\"\\nTraining on {} epoch completed.\\n\".format(epoch),\n",
        "              \"Average bpr_loss on train set is {} for the current epoch.\\n\".format(round(float(loss_sum/len(samples_train)), 6)),\n",
        "              \"Training top K precision = {}, recall = {}.\\n\".format(train_topk_precision, train_topk_recall),\n",
        "              \"Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n\".format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n",
        "                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n",
        "              \"Validation top K precision = {}, recall = {}.\\n\".format(val_topk_precision, val_topk_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a6319d5-861d-4535-c7e8-884c1c2031f4",
        "id": "9F2yBKF4Mh_2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Users: 200\n",
            "#Items: 3883\n",
            "use NORMAL distribution initilizer\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.9314208030700684 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.858711004257202 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 5.844376802444458 seconds)=====\n",
            "#Training samples: 100000 #Validation samples: 100000 #Test samples: 100000\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Training on the 0 epoch\n",
            "Training on epoch 0 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.788428, and regularization loss is 0.095287.\n",
            " Top K precision = 0.0846938775510204, recall = 0.007453607537872088.\n",
            "Training on epoch 0 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.737513, and regularization loss is 0.044366.\n",
            " Top K precision = 0.09354838709677414, recall = 0.008151970414416905.\n",
            "Training on epoch 0 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.716148, and regularization loss is 0.023001.\n",
            " Top K precision = 0.08131868131868131, recall = 0.007661306909293055.\n",
            "Training on epoch 0 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.706422, and regularization loss is 0.013275.\n",
            " Top K precision = 0.08139534883720928, recall = 0.007529712652323201.\n",
            "Training on epoch 0 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.701225, and regularization loss is 0.008078.\n",
            " Top K precision = 0.08444444444444445, recall = 0.007752215742691096.\n",
            "Training on epoch 0 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.699035, and regularization loss is 0.005888.\n",
            " Top K precision = 0.0939393939393939, recall = 0.009042967681880908.\n",
            "Training on epoch 0 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.697475, and regularization loss is 0.004328.\n",
            " Top K precision = 0.08762886597938141, recall = 0.007686808063545044.\n",
            "Training on epoch 0 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.697107, and regularization loss is 0.00396.\n",
            " Top K precision = 0.07065217391304343, recall = 0.005249993650636543.\n",
            "\n",
            "Training on 0 epoch completed.\n",
            " Average bpr_loss on train set is 0.00557 for the current epoch.\n",
            " Training top K precision = 0.04799999999999997, recall = 0.003394365996241671.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.032999999999999974, recall = 0.0027094075620177926.\n",
            "\n",
            "Training on the 1 epoch\n",
            "Training on epoch 1 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.695044, and regularization loss is 0.001897.\n",
            " Top K precision = 0.08021978021978017, recall = 0.00914833787571286.\n",
            "Training on epoch 1 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.695389, and regularization loss is 0.002242.\n",
            " Top K precision = 0.09148936170212761, recall = 0.008330928348587412.\n",
            "Training on epoch 1 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69502, and regularization loss is 0.001873.\n",
            " Top K precision = 0.10816326530612239, recall = 0.007850699035620634.\n",
            "Training on epoch 1 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.694218, and regularization loss is 0.001071.\n",
            " Top K precision = 0.09010989010989008, recall = 0.008133275290186719.\n",
            "Training on epoch 1 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693918, and regularization loss is 0.00077.\n",
            " Top K precision = 0.0947368421052631, recall = 0.009586321847189511.\n",
            "Training on epoch 1 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.694354, and regularization loss is 0.001207.\n",
            " Top K precision = 0.09120879120879118, recall = 0.007279408272413932.\n",
            "Training on epoch 1 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69379, and regularization loss is 0.000642.\n",
            " Top K precision = 0.08958333333333329, recall = 0.007922601615139377.\n",
            "Training on epoch 1 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.694018, and regularization loss is 0.000871.\n",
            " Top K precision = 0.09450549450549449, recall = 0.007507457432789601.\n",
            "\n",
            "Training on 1 epoch completed.\n",
            " Average bpr_loss on train set is 0.00543 for the current epoch.\n",
            " Training top K precision = 0.05899999999999996, recall = 0.006169794013597854.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.060999999999999936, recall = 0.005873412439121267.\n",
            "\n",
            "Training on the 2 epoch\n",
            "Training on epoch 2 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693505, and regularization loss is 0.000358.\n",
            " Top K precision = 0.10505050505050502, recall = 0.0086172800673892.\n",
            "Training on epoch 2 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693622, and regularization loss is 0.000475.\n",
            " Top K precision = 0.08020833333333331, recall = 0.006117004744181641.\n",
            "Training on epoch 2 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693686, and regularization loss is 0.000539.\n",
            " Top K precision = 0.07916666666666662, recall = 0.006536762464491959.\n",
            "Training on epoch 2 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693532, and regularization loss is 0.000385.\n",
            " Top K precision = 0.07831325301204818, recall = 0.006911379435484154.\n",
            "Training on epoch 2 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693718, and regularization loss is 0.000571.\n",
            " Top K precision = 0.09782608695652172, recall = 0.007852860189319485.\n",
            "Training on epoch 2 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693437, and regularization loss is 0.000289.\n",
            " Top K precision = 0.09684210526315788, recall = 0.007880096019262275.\n",
            "Training on epoch 2 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693471, and regularization loss is 0.000324.\n",
            " Top K precision = 0.09595959595959594, recall = 0.007813453249219088.\n",
            "Training on epoch 2 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693399, and regularization loss is 0.000252.\n",
            " Top K precision = 0.08510638297872336, recall = 0.007440569349121044.\n",
            "\n",
            "Training on 2 epoch completed.\n",
            " Average bpr_loss on train set is 0.005423 for the current epoch.\n",
            " Training top K precision = 0.061499999999999944, recall = 0.005758273574205882.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.06399999999999995, recall = 0.006195320455021263.\n",
            "\n",
            "Training on the 3 epoch\n",
            "Training on epoch 3 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693446, and regularization loss is 0.000299.\n",
            " Top K precision = 0.07684210526315785, recall = 0.006468243439147211.\n",
            "Training on epoch 3 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693351, and regularization loss is 0.000204.\n",
            " Top K precision = 0.08737864077669896, recall = 0.008184808495007726.\n",
            "Training on epoch 3 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693329, and regularization loss is 0.000182.\n",
            " Top K precision = 0.09111111111111106, recall = 0.007835108370773282.\n",
            "Training on epoch 3 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693317, and regularization loss is 0.000169.\n",
            " Top K precision = 0.10430107526881711, recall = 0.008408398125479324.\n",
            "Training on epoch 3 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693445, and regularization loss is 0.000297.\n",
            " Top K precision = 0.07599999999999998, recall = 0.00632497999942511.\n",
            "Training on epoch 3 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693303, and regularization loss is 0.000156.\n",
            " Top K precision = 0.09213483146067411, recall = 0.006970630672081871.\n",
            "Training on epoch 3 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693311, and regularization loss is 0.000163.\n",
            " Top K precision = 0.09354838709677417, recall = 0.008938440263906649.\n",
            "Training on epoch 3 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693301, and regularization loss is 0.000154.\n",
            " Top K precision = 0.08297872340425529, recall = 0.007158206774094596.\n",
            "\n",
            "Training on 3 epoch completed.\n",
            " Average bpr_loss on train set is 0.005422 for the current epoch.\n",
            " Training top K precision = 0.06099999999999994, recall = 0.00567743973166141.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07549999999999994, recall = 0.006943313185243842.\n",
            "\n",
            "Training on the 4 epoch\n",
            "Training on epoch 4 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08631578947368418, recall = 0.007177526972569731.\n",
            "Training on epoch 4 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693219, and regularization loss is 7.2e-05.\n",
            " Top K precision = 0.06734693877551018, recall = 0.006688561523080308.\n",
            "Training on epoch 4 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693189, and regularization loss is 4.1e-05.\n",
            " Top K precision = 0.08602150537634408, recall = 0.006811676442689706.\n",
            "Training on epoch 4 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693221, and regularization loss is 7.4e-05.\n",
            " Top K precision = 0.10515463917525772, recall = 0.007166135309775272.\n",
            "Training on epoch 4 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693266, and regularization loss is 0.000118.\n",
            " Top K precision = 0.09381443298969067, recall = 0.008067269349982557.\n",
            "Training on epoch 4 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693188, and regularization loss is 4e-05.\n",
            " Top K precision = 0.09479166666666662, recall = 0.008002327047155455.\n",
            "Training on epoch 4 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693182, and regularization loss is 3.5e-05.\n",
            " Top K precision = 0.08787878787878783, recall = 0.008814965806579082.\n",
            "Training on epoch 4 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693209, and regularization loss is 6.1e-05.\n",
            " Top K precision = 0.0895833333333333, recall = 0.008752880534370669.\n",
            "\n",
            "Training on 4 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.05649999999999994, recall = 0.005194315551895796.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.042499999999999975, recall = 0.004381116147838079.\n",
            "\n",
            "Training on the 5 epoch\n",
            "Training on epoch 5 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693209, and regularization loss is 6.2e-05.\n",
            " Top K precision = 0.09052631578947362, recall = 0.008272958039769067.\n",
            "Training on epoch 5 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693226, and regularization loss is 7.9e-05.\n",
            " Top K precision = 0.08723404255319146, recall = 0.006148865336728459.\n",
            "Training on epoch 5 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693212, and regularization loss is 6.4e-05.\n",
            " Top K precision = 0.08217821782178215, recall = 0.005652426693035331.\n",
            "Training on epoch 5 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.7e-05.\n",
            " Top K precision = 0.08202247191011233, recall = 0.00682723904114098.\n",
            "Training on epoch 5 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693201, and regularization loss is 5.3e-05.\n",
            " Top K precision = 0.08617021276595742, recall = 0.00900909696222922.\n",
            "Training on epoch 5 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.08865979381443295, recall = 0.006673309966341345.\n",
            "Training on epoch 5 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693219, and regularization loss is 7.2e-05.\n",
            " Top K precision = 0.07812499999999996, recall = 0.0077242756753605835.\n",
            "Training on epoch 5 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693187, and regularization loss is 3.9e-05.\n",
            " Top K precision = 0.09684210526315788, recall = 0.00753073221929097.\n",
            "\n",
            "Training on 5 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.06949999999999991, recall = 0.006226243767562712.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07049999999999992, recall = 0.006321774211710932.\n",
            "\n",
            "Training on the 6 epoch\n",
            "Training on epoch 6 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693178, and regularization loss is 3.1e-05.\n",
            " Top K precision = 0.0864583333333333, recall = 0.009216706317511664.\n",
            "Training on epoch 6 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.098989898989899, recall = 0.008197489199137263.\n",
            "Training on epoch 6 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693196, and regularization loss is 4.8e-05.\n",
            " Top K precision = 0.08041237113402058, recall = 0.007897916134188313.\n",
            "Training on epoch 6 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.08804347826086956, recall = 0.008469405012698536.\n",
            "Training on epoch 6 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09484536082474224, recall = 0.007324165492432905.\n",
            "Training on epoch 6 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.09684210526315785, recall = 0.006276955073115431.\n",
            "Training on epoch 6 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693199, and regularization loss is 5.1e-05.\n",
            " Top K precision = 0.09494949494949495, recall = 0.006304447138625227.\n",
            "Training on epoch 6 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693191, and regularization loss is 4.3e-05.\n",
            " Top K precision = 0.09183673469387751, recall = 0.007863168184644359.\n",
            "\n",
            "Training on 6 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.06599999999999993, recall = 0.006284318105551431.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.05899999999999992, recall = 0.0053093552989058835.\n",
            "\n",
            "Training on the 7 epoch\n",
            "Training on epoch 7 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09479166666666662, recall = 0.008235188757632851.\n",
            "Training on epoch 7 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.09999999999999994, recall = 0.008030189836168326.\n",
            "Training on epoch 7 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08762886597938142, recall = 0.00865007301502351.\n",
            "Training on epoch 7 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09325842696629208, recall = 0.007209125997860434.\n",
            "Training on epoch 7 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 8e-06.\n",
            " Top K precision = 0.08247422680412368, recall = 0.00800039831217717.\n",
            "Training on epoch 7 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0989583333333333, recall = 0.008842238670637657.\n",
            "Training on epoch 7 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08387096774193545, recall = 0.0071438079456249606.\n",
            "Training on epoch 7 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693152, and regularization loss is 5e-06.\n",
            " Top K precision = 0.10430107526881716, recall = 0.00848361836846752.\n",
            "\n",
            "Training on 7 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.050499999999999955, recall = 0.004492400189247509.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.041499999999999974, recall = 0.004074330030814403.\n",
            "\n",
            "Training on the 8 epoch\n",
            "Training on epoch 8 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.08723404255319146, recall = 0.006624772883840347.\n",
            "Training on epoch 8 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09687499999999998, recall = 0.007112125881002692.\n",
            "Training on epoch 8 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693184, and regularization loss is 3.7e-05.\n",
            " Top K precision = 0.08913043478260867, recall = 0.006456963055976592.\n",
            "Training on epoch 8 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08723404255319146, recall = 0.007271130620096692.\n",
            "Training on epoch 8 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09888888888888886, recall = 0.008114146020713529.\n",
            "Training on epoch 8 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 6e-06.\n",
            " Top K precision = 0.08494623655913977, recall = 0.007664319317143975.\n",
            "Training on epoch 8 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.10107526881720424, recall = 0.00803713441110293.\n",
            "Training on epoch 8 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.07291666666666667, recall = 0.006773780115997657.\n",
            "\n",
            "Training on 8 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.04899999999999994, recall = 0.0045092420755419495.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.04899999999999995, recall = 0.004583567196573618.\n",
            "\n",
            "Training on the 9 epoch\n",
            "Training on epoch 9 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.10555555555555551, recall = 0.008477593971161051.\n",
            "Training on epoch 9 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.07978723404255317, recall = 0.006143135075701594.\n",
            "Training on epoch 9 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.096039603960396, recall = 0.009039069820489682.\n",
            "Training on epoch 9 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08617021276595745, recall = 0.0071233558999742146.\n",
            "Training on epoch 9 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08437499999999996, recall = 0.007979493156186105.\n",
            "Training on epoch 9 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09797979797979796, recall = 0.007296610101229357.\n",
            "Training on epoch 9 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.05869565217391304, recall = 0.004934550082012139.\n",
            "Training on epoch 9 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09468085106382977, recall = 0.007076561689729507.\n",
            "\n",
            "Training on 9 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.06199999999999992, recall = 0.005862233452076998.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.05499999999999993, recall = 0.004958340890718339.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model built using technique 3"
      ],
      "metadata": {
        "id": "C5-DcKwEddvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.getcwd()\n",
        "movielens = MovieLens(root=root, transform=trans_ml)\n",
        "data = movielens.get()\n",
        "train_mask, val_mask, test_mask = \\\n",
        "        movielens.train_val_test_split(val_frac=config_dict[\"val_frac\"],\n",
        "                                       test_frac=config_dict[\"test_frac\"])\n",
        "\n",
        "n_users = len(data[\"users\"].unique())\n",
        "m_items = len(data[\"items\"].unique())\n",
        "print(f\"#Users: {n_users}\")\n",
        "print(f\"#Items: {m_items}\")\n",
        "\n",
        "model_config = {\n",
        "    \"n_users\": n_users,\n",
        "    \"m_items\": m_items,\n",
        "    \"embedding_size\": config_dict[\"embedding_size\"],\n",
        "    \"num_layers\": config_dict[\"num_layers\"],\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lightGCN3 = LightGCN3(model_config, device=device)\n",
        "\n",
        "num_samples_per_user = config_dict[\"num_samples_per_user\"]\n",
        "epochs = config_dict[\"epochs\"]\n",
        "batch_size = config_dict[\"batch_size\"]\n",
        "lr = config_dict[\"lr\"]\n",
        "weight_decay = config_dict[\"weight_decay\"]\n",
        "\n",
        "K = config_dict[\"K\"]\n",
        "\n",
        "lightGCN3.to(device)\n",
        "\n",
        "samples_train, samples_val, samples_test = \\\n",
        "        sample_pos_neg(data, train_mask, val_mask, test_mask,\n",
        "                       num_samples_per_user)\n",
        "\n",
        "samples_train=samples_train.to(device)\n",
        "samples_val=samples_val.to(device)\n",
        "samples_test=samples_test.to(device)\n",
        "train_mask=train_mask.to(device)\n",
        "val_mask=val_mask.to(device)\n",
        "test_mask=test_mask.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "print(f\"#Training samples: {len(samples_train)}\",\n",
        "      f\"#Validation samples: {len(samples_val)}\",\n",
        "      f\"#Test samples: {len(samples_test)}\")\n",
        "\n",
        "optimizer = optim.Adam(lightGCN3.parameters(), lr=lr)\n",
        "print(\"Optimizer:\", optimizer)\n",
        "\n",
        "epochs_tracked = []\n",
        "train_topks3 = []\n",
        "val_topks3 = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Training on the {} epoch\".format(epoch))\n",
        "    lightGCN3.train()\n",
        "    loss_sum = 0\n",
        "    # Shuffle the order of rows.\n",
        "    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n",
        "    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        current_batch = \\\n",
        "            samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
        "        # Shuffle the order of rows.\n",
        "        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n",
        "        users = current_batch[:, 0:1]\n",
        "        pos = current_batch[:, 1:2]\n",
        "        neg = current_batch[:, 2:3]\n",
        "\n",
        "        loss, reg_loss = bpr_loss(lightGCN3, users, pos, neg, data,\n",
        "                                  train_mask)\n",
        "        reg_loss = reg_loss * weight_decay\n",
        "        loss = loss + reg_loss\n",
        "        loss_sum += loss.detach()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % config_dict[\"minibatch_per_print\"] == 0:\n",
        "            all_users = torch.linspace(start=0,\n",
        "                                       end=n_users - 1, steps=n_users).long()\n",
        "            user_indices = current_batch[:, 0]\n",
        "            user_indices = user_indices.repeat(2).long()\n",
        "            item_indices = torch.cat(\n",
        "                (current_batch[:, 1], current_batch[:, 2])).long()\n",
        "            pred = getUsersRating(lightGCN3,\n",
        "                                  all_users,\n",
        "                                  data)[user_indices, item_indices]\n",
        "            truth = data[\"edge_index\"][user_indices, item_indices]\n",
        "            topk_precision, topk_recall = \\\n",
        "                personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "\n",
        "            print(\"Training on epoch {} minibatch {}/{} completed\\n\".format(epoch, batch_idx+1,\n",
        "                                                                            math.ceil(len(samples_train) / batch_size)),\n",
        "                  \"bpr_loss on current minibatch is {}, and regularization loss is {}.\\n\".format(round(float(loss.detach().cpu()), 6),\n",
        "                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n",
        "                  \"Top K precision = {}, recall = {}.\".format(topk_precision, topk_recall))\n",
        "\n",
        "    if epoch % config_dict[\"epochs_per_print\"] == 0:\n",
        "        epochs_tracked.append(epoch)\n",
        "\n",
        "        # evaluation on both the trainisng and validation set\n",
        "        lightGCN3.eval()\n",
        "        # predict on the training set\n",
        "        users = samples_train[:, 0:1]\n",
        "        user_indices = samples_train[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat(\n",
        "            (samples_train[:, 1], samples_train[:, 2])).long()\n",
        "        pred = getUsersRating(lightGCN3,\n",
        "                              users[:,0],\n",
        "                              data)[user_indices, item_indices]\n",
        "        truth = data[\"edge_index\"][users.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        train_topk_precision, train_topk_recall = \\\n",
        "            personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "        train_topks3.append((train_topk_precision, train_topk_recall))\n",
        "\n",
        "        # predict on the validation set\n",
        "        users_val = samples_val[:, 0:1]\n",
        "        pos_val = samples_val[:, 1:2]\n",
        "        neg_val = samples_val[:, 2:3]\n",
        "\n",
        "        loss_val, reg_loss_val = bpr_loss(\n",
        "            lightGCN3, users_val, pos_val, neg_val, data, val_mask)\n",
        "        reg_loss_val = reg_loss_val * weight_decay\n",
        "\n",
        "        # predict on the validation set\n",
        "        user_indices = samples_val[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n",
        "        pred_val = getUsersRating(lightGCN3,\n",
        "                                  users_val[:,0],\n",
        "                                  data)[user_indices, item_indices]\n",
        "        truth_val = data[\"edge_index\"][users_val.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        val_topk_precision, val_topk_recall = \\\n",
        "            personalized_topk(pred_val, K, user_indices, data[\"edge_index\"])\n",
        "        val_topks3.append((val_topk_precision, val_topk_recall))\n",
        "\n",
        "        print(\"\\nTraining on {} epoch completed.\\n\".format(epoch),\n",
        "              \"Average bpr_loss on train set is {} for the current epoch.\\n\".format(round(float(loss_sum/len(samples_train)), 6)),\n",
        "              \"Training top K precision = {}, recall = {}.\\n\".format(train_topk_precision, train_topk_recall),\n",
        "              \"Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n\".format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n",
        "                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n",
        "              \"Validation top K precision = {}, recall = {}.\\n\".format(val_topk_precision, val_topk_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1515b3b-fcd0-4fcd-e3a8-8e66e6ba8c8a",
        "id": "cQvNt4K5PnGi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Users: 200\n",
            "#Items: 3883\n",
            "use NORMAL distribution initilizer\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.5524044036865234 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.6534972190856934 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 3.2234461307525635 seconds)=====\n",
            "#Training samples: 100000 #Validation samples: 100000 #Test samples: 100000\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Training on the 0 epoch\n",
            "Training on epoch 0 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.791276, and regularization loss is 0.098129.\n",
            " Top K precision = 0.10606060606060602, recall = 0.008642748024121454.\n",
            "Training on epoch 0 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.739603, and regularization loss is 0.046456.\n",
            " Top K precision = 0.09772727272727275, recall = 0.0068620603104758236.\n",
            "Training on epoch 0 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.718279, and regularization loss is 0.025132.\n",
            " Top K precision = 0.08817204301075267, recall = 0.007362952516200392.\n",
            "Training on epoch 0 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.707956, and regularization loss is 0.014808.\n",
            " Top K precision = 0.10618556701030923, recall = 0.008159531279376629.\n",
            "Training on epoch 0 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.700721, and regularization loss is 0.007574.\n",
            " Top K precision = 0.09670329670329668, recall = 0.008569244416901342.\n",
            "Training on epoch 0 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69822, and regularization loss is 0.005073.\n",
            " Top K precision = 0.08969072164948447, recall = 0.00743831025810805.\n",
            "Training on epoch 0 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.697114, and regularization loss is 0.003966.\n",
            " Top K precision = 0.10303030303030296, recall = 0.008753142759479379.\n",
            "Training on epoch 0 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.696257, and regularization loss is 0.00311.\n",
            " Top K precision = 0.10309278350515456, recall = 0.007069399450002976.\n",
            "\n",
            "Training on 0 epoch completed.\n",
            " Average bpr_loss on train set is 0.005569 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007558772127474777.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 1 epoch\n",
            "Training on epoch 1 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.695987, and regularization loss is 0.002839.\n",
            " Top K precision = 0.09213483146067412, recall = 0.007764182854640439.\n",
            "Training on epoch 1 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.695005, and regularization loss is 0.001858.\n",
            " Top K precision = 0.08041237113402057, recall = 0.006765982879961843.\n",
            "Training on epoch 1 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.694639, and regularization loss is 0.001492.\n",
            " Top K precision = 0.08842105263157891, recall = 0.008640382587730354.\n",
            "Training on epoch 1 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.694403, and regularization loss is 0.001255.\n",
            " Top K precision = 0.08297872340425529, recall = 0.007401047108225005.\n",
            "Training on epoch 1 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.694116, and regularization loss is 0.000969.\n",
            " Top K precision = 0.08080808080808081, recall = 0.005966645706871879.\n",
            "Training on epoch 1 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.694317, and regularization loss is 0.001169.\n",
            " Top K precision = 0.09325842696629211, recall = 0.007650221054038661.\n",
            "Training on epoch 1 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69413, and regularization loss is 0.000983.\n",
            " Top K precision = 0.07362637362637363, recall = 0.005965120690937816.\n",
            "Training on epoch 1 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693999, and regularization loss is 0.000852.\n",
            " Top K precision = 0.09569892473118274, recall = 0.007964409795949427.\n",
            "\n",
            "Training on 1 epoch completed.\n",
            " Average bpr_loss on train set is 0.005429 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 2 epoch\n",
            "Training on epoch 2 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693502, and regularization loss is 0.000355.\n",
            " Top K precision = 0.0852941176470588, recall = 0.007349076624261126.\n",
            "Training on epoch 2 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693778, and regularization loss is 0.000631.\n",
            " Top K precision = 0.09270833333333328, recall = 0.007207292131735792.\n",
            "Training on epoch 2 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693451, and regularization loss is 0.000304.\n",
            " Top K precision = 0.07789473684210525, recall = 0.005827016946201105.\n",
            "Training on epoch 2 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693383, and regularization loss is 0.000236.\n",
            " Top K precision = 0.07032967032967033, recall = 0.006662762810326552.\n",
            "Training on epoch 2 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693405, and regularization loss is 0.000258.\n",
            " Top K precision = 0.08602150537634404, recall = 0.006332841755689846.\n",
            "Training on epoch 2 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693419, and regularization loss is 0.000272.\n",
            " Top K precision = 0.10198019801980197, recall = 0.007902223068622446.\n",
            "Training on epoch 2 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000112.\n",
            " Top K precision = 0.0978260869565217, recall = 0.007190428265111904.\n",
            "Training on epoch 2 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693567, and regularization loss is 0.000419.\n",
            " Top K precision = 0.08969072164948447, recall = 0.007261744622517031.\n",
            "\n",
            "Training on 2 epoch completed.\n",
            " Average bpr_loss on train set is 0.005423 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 3 epoch\n",
            "Training on epoch 3 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693377, and regularization loss is 0.000229.\n",
            " Top K precision = 0.09042553191489358, recall = 0.00813219640867872.\n",
            "Training on epoch 3 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693238, and regularization loss is 9.1e-05.\n",
            " Top K precision = 0.08181818181818179, recall = 0.006691549177808403.\n",
            "Training on epoch 3 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69346, and regularization loss is 0.000313.\n",
            " Top K precision = 0.1048543689320388, recall = 0.0074801088334163.\n",
            "Training on epoch 3 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693343, and regularization loss is 0.000196.\n",
            " Top K precision = 0.086734693877551, recall = 0.005968296539652152.\n",
            "Training on epoch 3 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693254, and regularization loss is 0.000106.\n",
            " Top K precision = 0.08064516129032256, recall = 0.007827726609955973.\n",
            "Training on epoch 3 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693345, and regularization loss is 0.000198.\n",
            " Top K precision = 0.10416666666666669, recall = 0.010023212005887445.\n",
            "Training on epoch 3 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693305, and regularization loss is 0.000158.\n",
            " Top K precision = 0.08977272727272723, recall = 0.00653580814069614.\n",
            "Training on epoch 3 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693205, and regularization loss is 5.8e-05.\n",
            " Top K precision = 0.09680851063829783, recall = 0.008655400973539978.\n",
            "\n",
            "Training on 3 epoch completed.\n",
            " Average bpr_loss on train set is 0.005422 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 4 epoch\n",
            "Training on epoch 4 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693397, and regularization loss is 0.000249.\n",
            " Top K precision = 0.08989898989898988, recall = 0.00904011535395346.\n",
            "Training on epoch 4 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
            " Top K precision = 0.08421052631578943, recall = 0.008027839921679065.\n",
            "Training on epoch 4 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693258, and regularization loss is 0.000111.\n",
            " Top K precision = 0.07575757575757572, recall = 0.006758103705854156.\n",
            "Training on epoch 4 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693211, and regularization loss is 6.4e-05.\n",
            " Top K precision = 0.07812499999999999, recall = 0.007429619984864476.\n",
            "Training on epoch 4 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693262, and regularization loss is 0.000115.\n",
            " Top K precision = 0.0901960784313725, recall = 0.0062474291382209985.\n",
            "Training on epoch 4 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69327, and regularization loss is 0.000123.\n",
            " Top K precision = 0.08105263157894738, recall = 0.007554422920127434.\n",
            "Training on epoch 4 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693293, and regularization loss is 0.000146.\n",
            " Top K precision = 0.08865979381443292, recall = 0.008229013734343905.\n",
            "Training on epoch 4 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693255, and regularization loss is 0.000108.\n",
            " Top K precision = 0.09021739130434779, recall = 0.008404671669367838.\n",
            "\n",
            "Training on 4 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 5 epoch\n",
            "Training on epoch 5 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.09207920792079202, recall = 0.006666601333827941.\n",
            "Training on epoch 5 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693228, and regularization loss is 8e-05.\n",
            " Top K precision = 0.08842105263157891, recall = 0.005010537332902192.\n",
            "Training on epoch 5 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09999999999999999, recall = 0.007675182392835622.\n",
            "Training on epoch 5 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693184, and regularization loss is 3.7e-05.\n",
            " Top K precision = 0.0857142857142857, recall = 0.008595784570726805.\n",
            "Training on epoch 5 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693222, and regularization loss is 7.5e-05.\n",
            " Top K precision = 0.06597938144329896, recall = 0.0062403877361564145.\n",
            "Training on epoch 5 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69326, and regularization loss is 0.000113.\n",
            " Top K precision = 0.07551020408163264, recall = 0.007330302981757139.\n",
            "Training on epoch 5 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.08555555555555554, recall = 0.009036401839186461.\n",
            "Training on epoch 5 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.0836734693877551, recall = 0.006306942289302934.\n",
            "\n",
            "Training on 5 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 6 epoch\n",
            "Training on epoch 6 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.0783505154639175, recall = 0.006747650763429281.\n",
            "Training on epoch 6 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693195, and regularization loss is 4.7e-05.\n",
            " Top K precision = 0.08775510204081632, recall = 0.007250541262963391.\n",
            "Training on epoch 6 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08399999999999996, recall = 0.007884800973211507.\n",
            "Training on epoch 6 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693174, and regularization loss is 2.7e-05.\n",
            " Top K precision = 0.08372093023255812, recall = 0.007422292307336214.\n",
            "Training on epoch 6 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69318, and regularization loss is 3.3e-05.\n",
            " Top K precision = 0.08265306122448976, recall = 0.00759144289836722.\n",
            "Training on epoch 6 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693199, and regularization loss is 5.1e-05.\n",
            " Top K precision = 0.08636363636363635, recall = 0.007244328473439454.\n",
            "Training on epoch 6 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693178, and regularization loss is 3e-05.\n",
            " Top K precision = 0.09263157894736841, recall = 0.008226300902116526.\n",
            "Training on epoch 6 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
            " Top K precision = 0.083695652173913, recall = 0.008110383041991008.\n",
            "\n",
            "Training on 6 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999991, recall = 0.007558772127474772.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 7 epoch\n",
            "Training on epoch 7 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08453608247422678, recall = 0.007608537138158372.\n",
            "Training on epoch 7 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0863157894736842, recall = 0.008328971819215237.\n",
            "Training on epoch 7 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693153, and regularization loss is 5e-06.\n",
            " Top K precision = 0.0792079207920792, recall = 0.007043180777121634.\n",
            "Training on epoch 7 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693179, and regularization loss is 3.2e-05.\n",
            " Top K precision = 0.10103092783505149, recall = 0.008413718069830609.\n",
            "Training on epoch 7 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693245, and regularization loss is 9.8e-05.\n",
            " Top K precision = 0.07912087912087908, recall = 0.0066188207262089325.\n",
            "Training on epoch 7 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
            " Top K precision = 0.08989898989898985, recall = 0.0075447051389286824.\n",
            "Training on epoch 7 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.09789473684210526, recall = 0.007816493418972921.\n",
            "Training on epoch 7 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08777777777777779, recall = 0.006213737427960162.\n",
            "\n",
            "Training on 7 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007558772127474773.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 8 epoch\n",
            "Training on epoch 8 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.09583333333333327, recall = 0.009151292171617226.\n",
            "Training on epoch 8 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.08452380952380951, recall = 0.00706660739092336.\n",
            "Training on epoch 8 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.07999999999999997, recall = 0.005888978214842924.\n",
            "Training on epoch 8 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.0776595744680851, recall = 0.006717807345680286.\n",
            "Training on epoch 8 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.06551724137931035, recall = 0.0065051820986910085.\n",
            "Training on epoch 8 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.0896907216494845, recall = 0.0074637854260741.\n",
            "Training on epoch 8 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08775510204081631, recall = 0.006930975944017429.\n",
            "Training on epoch 8 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09340659340659337, recall = 0.008774527122768628.\n",
            "\n",
            "Training on 8 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999991, recall = 0.00755877212747478.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 9 epoch\n",
            "Training on epoch 9 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09099999999999993, recall = 0.007469410270245256.\n",
            "Training on epoch 9 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09560439560439558, recall = 0.007206028360954933.\n",
            "Training on epoch 9 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08247422680412367, recall = 0.008087873441211164.\n",
            "Training on epoch 9 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693184, and regularization loss is 3.7e-05.\n",
            " Top K precision = 0.07959183673469385, recall = 0.00771712827557334.\n",
            "Training on epoch 9 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09081632653061218, recall = 0.008373428869056482.\n",
            "Training on epoch 9 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.07575757575757575, recall = 0.0064547804363459605.\n",
            "Training on epoch 9 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08999999999999993, recall = 0.008196920557934556.\n",
            "Training on epoch 9 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.09587628865979378, recall = 0.007178831300055943.\n",
            "\n",
            "Training on 9 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.00755877212747478.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model built using technique 4"
      ],
      "metadata": {
        "id": "5TtnyqrQdjIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.getcwd()\n",
        "movielens = MovieLens(root=root, transform=trans_ml)\n",
        "data = movielens.get()\n",
        "train_mask, val_mask, test_mask = \\\n",
        "        movielens.train_val_test_split(val_frac=config_dict[\"val_frac\"],\n",
        "                                       test_frac=config_dict[\"test_frac\"])\n",
        "\n",
        "n_users = len(data[\"users\"].unique())\n",
        "m_items = len(data[\"items\"].unique())\n",
        "print(f\"#Users: {n_users}\")\n",
        "print(f\"#Items: {m_items}\")\n",
        "\n",
        "model_config = {\n",
        "    \"n_users\": n_users,\n",
        "    \"m_items\": m_items,\n",
        "    \"embedding_size\": config_dict[\"embedding_size\"],\n",
        "    \"num_layers\": config_dict[\"num_layers\"],\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lightGCN4 = LightGCN4(model_config, device=device)\n",
        "\n",
        "num_samples_per_user = config_dict[\"num_samples_per_user\"]\n",
        "epochs = config_dict[\"epochs\"]\n",
        "batch_size = config_dict[\"batch_size\"]\n",
        "lr = config_dict[\"lr\"]\n",
        "weight_decay = config_dict[\"weight_decay\"]\n",
        "\n",
        "K = config_dict[\"K\"]\n",
        "\n",
        "lightGCN4.to(device)\n",
        "\n",
        "samples_train, samples_val, samples_test = \\\n",
        "        sample_pos_neg(data, train_mask, val_mask, test_mask,\n",
        "                       num_samples_per_user)\n",
        "\n",
        "samples_train=samples_train.to(device)\n",
        "samples_val=samples_val.to(device)\n",
        "samples_test=samples_test.to(device)\n",
        "train_mask=train_mask.to(device)\n",
        "val_mask=val_mask.to(device)\n",
        "test_mask=test_mask.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "print(f\"#Training samples: {len(samples_train)}\",\n",
        "      f\"#Validation samples: {len(samples_val)}\",\n",
        "      f\"#Test samples: {len(samples_test)}\")\n",
        "\n",
        "optimizer = optim.Adam(lightGCN4.parameters(), lr=lr)\n",
        "print(\"Optimizer:\", optimizer)\n",
        "\n",
        "epochs_tracked = []\n",
        "train_topks4 = []\n",
        "val_topks4 = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Training on the {} epoch\".format(epoch))\n",
        "    lightGCN4.train()\n",
        "    loss_sum = 0\n",
        "    # Shuffle the order of rows.\n",
        "    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n",
        "    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        current_batch = \\\n",
        "            samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
        "        # Shuffle the order of rows.\n",
        "        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n",
        "        users = current_batch[:, 0:1]\n",
        "        pos = current_batch[:, 1:2]\n",
        "        neg = current_batch[:, 2:3]\n",
        "\n",
        "        loss, reg_loss = bpr_loss(lightGCN4, users, pos, neg, data,\n",
        "                                  train_mask)\n",
        "        reg_loss = reg_loss * weight_decay\n",
        "        loss = loss + reg_loss\n",
        "        loss_sum += loss.detach()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % config_dict[\"minibatch_per_print\"] == 0:\n",
        "            all_users = torch.linspace(start=0,\n",
        "                                       end=n_users - 1, steps=n_users).long()\n",
        "            user_indices = current_batch[:, 0]\n",
        "            user_indices = user_indices.repeat(2).long()\n",
        "            item_indices = torch.cat(\n",
        "                (current_batch[:, 1], current_batch[:, 2])).long()\n",
        "            pred = getUsersRating(lightGCN4,\n",
        "                                  all_users,\n",
        "                                  data)[user_indices, item_indices]\n",
        "            truth = data[\"edge_index\"][user_indices, item_indices]\n",
        "            topk_precision, topk_recall = \\\n",
        "                personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "\n",
        "            print(\"Training on epoch {} minibatch {}/{} completed\\n\".format(epoch, batch_idx+1,\n",
        "                                                                            math.ceil(len(samples_train) / batch_size)),\n",
        "                  \"bpr_loss on current minibatch is {}, and regularization loss is {}.\\n\".format(round(float(loss.detach().cpu()), 6),\n",
        "                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n",
        "                  \"Top K precision = {}, recall = {}.\".format(topk_precision, topk_recall))\n",
        "\n",
        "    if epoch % config_dict[\"epochs_per_print\"] == 0:\n",
        "        epochs_tracked.append(epoch)\n",
        "\n",
        "        # evaluation on both the trainisng and validation set\n",
        "        lightGCN4.eval()\n",
        "        # predict on the training set\n",
        "        users = samples_train[:, 0:1]\n",
        "        user_indices = samples_train[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat(\n",
        "            (samples_train[:, 1], samples_train[:, 2])).long()\n",
        "        pred = getUsersRating(lightGCN4,\n",
        "                              users[:,0],\n",
        "                              data)[user_indices, item_indices]\n",
        "        truth = data[\"edge_index\"][users.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        train_topk_precision, train_topk_recall = \\\n",
        "            personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "        train_topks4.append((train_topk_precision, train_topk_recall))\n",
        "\n",
        "        # predict on the validation set\n",
        "        users_val = samples_val[:, 0:1]\n",
        "        pos_val = samples_val[:, 1:2]\n",
        "        neg_val = samples_val[:, 2:3]\n",
        "\n",
        "        loss_val, reg_loss_val = bpr_loss(\n",
        "            lightGCN4, users_val, pos_val, neg_val, data, val_mask)\n",
        "        reg_loss_val = reg_loss_val * weight_decay\n",
        "\n",
        "        # predict on the validation set\n",
        "        user_indices = samples_val[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n",
        "        pred_val = getUsersRating(lightGCN4,\n",
        "                                  users_val[:,0],\n",
        "                                  data)[user_indices, item_indices]\n",
        "        truth_val = data[\"edge_index\"][users_val.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        val_topk_precision, val_topk_recall = \\\n",
        "            personalized_topk(pred_val, K, user_indices, data[\"edge_index\"])\n",
        "        val_topks4.append((val_topk_precision, val_topk_recall))\n",
        "\n",
        "        print(\"\\nTraining on {} epoch completed.\\n\".format(epoch),\n",
        "              \"Average bpr_loss on train set is {} for the current epoch.\\n\".format(round(float(loss_sum/len(samples_train)), 6)),\n",
        "              \"Training top K precision = {}, recall = {}.\\n\".format(train_topk_precision, train_topk_recall),\n",
        "              \"Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n\".format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n",
        "                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n",
        "              \"Validation top K precision = {}, recall = {}.\\n\".format(val_topk_precision, val_topk_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a582275-8d0a-4aba-e741-c7494c7d826f",
        "id": "Ch0PPzL6Jh8g"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Users: 200\n",
            "#Items: 3883\n",
            "use NORMAL distribution initilizer\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.6988024711608887 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.7591781616210938 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.9951186180114746 seconds)=====\n",
            "#Training samples: 100000 #Validation samples: 100000 #Test samples: 100000\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Training on the 0 epoch\n",
            "Training on epoch 0 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 54.983093, and regularization loss is 0.096365.\n",
            " Top K precision = 0.10303030303030297, recall = 0.007883130980927766.\n",
            "Training on epoch 0 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.827293, and regularization loss is 0.082777.\n",
            " Top K precision = 0.07789473684210522, recall = 0.007282415734970405.\n",
            "Training on epoch 0 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.799269, and regularization loss is 0.077208.\n",
            " Top K precision = 0.09545454545454542, recall = 0.009322340972901754.\n",
            "Training on epoch 0 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.837846, and regularization loss is 0.067922.\n",
            " Top K precision = 0.08369565217391305, recall = 0.007840637194871157.\n",
            "Training on epoch 0 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.782266, and regularization loss is 0.062978.\n",
            " Top K precision = 0.06631578947368418, recall = 0.007308233822339426.\n",
            "Training on epoch 0 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.838743, and regularization loss is 0.059717.\n",
            " Top K precision = 0.09263157894736841, recall = 0.007187382706329884.\n",
            "Training on epoch 0 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.812665, and regularization loss is 0.053593.\n",
            " Top K precision = 0.07659574468085105, recall = 0.006426886287262291.\n",
            "Training on epoch 0 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.902594, and regularization loss is 0.05237.\n",
            " Top K precision = 0.10202020202020197, recall = 0.008258781907574278.\n",
            "\n",
            "Training on 0 epoch completed.\n",
            " Average bpr_loss on train set is 0.011806 for the current epoch.\n",
            " Training top K precision = 0.07749999999999992, recall = 0.007035510894666371.\n",
            " Average bpr_loss on the validation set is 5e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07749999999999992, recall = 0.006943477994360125.\n",
            "\n",
            "Training on the 1 epoch\n",
            "Training on epoch 1 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.80831, and regularization loss is 0.04754.\n",
            " Top K precision = 0.08556701030927834, recall = 0.006659860586025652.\n",
            "Training on epoch 1 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.762424, and regularization loss is 0.049488.\n",
            " Top K precision = 0.08723404255319146, recall = 0.008354922257122598.\n",
            "Training on epoch 1 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.769251, and regularization loss is 0.045835.\n",
            " Top K precision = 0.0848484848484848, recall = 0.008083651794038707.\n",
            "Training on epoch 1 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.75487, and regularization loss is 0.044198.\n",
            " Top K precision = 0.08526315789473678, recall = 0.007897008320877252.\n",
            "Training on epoch 1 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.920308, and regularization loss is 0.039849.\n",
            " Top K precision = 0.08173076923076922, recall = 0.007059846861576114.\n",
            "Training on epoch 1 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.75167, and regularization loss is 0.039218.\n",
            " Top K precision = 0.07444444444444444, recall = 0.007150189705094468.\n",
            "Training on epoch 1 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.753696, and regularization loss is 0.037885.\n",
            " Top K precision = 0.08437499999999996, recall = 0.008512611838855764.\n",
            "Training on epoch 1 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.75729, and regularization loss is 0.039844.\n",
            " Top K precision = 0.09374999999999996, recall = 0.00752974135044101.\n",
            "\n",
            "Training on 1 epoch completed.\n",
            " Average bpr_loss on train set is 0.006121 for the current epoch.\n",
            " Training top K precision = 0.07699999999999992, recall = 0.0064248642451393305.\n",
            " Average bpr_loss on the validation set is 4.4e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07799999999999992, recall = 0.007004453604116222.\n",
            "\n",
            "Training on the 2 epoch\n",
            "Training on epoch 2 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.970584, and regularization loss is 0.03736.\n",
            " Top K precision = 0.09148936170212761, recall = 0.008504806387166046.\n",
            "Training on epoch 2 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.75077, and regularization loss is 0.035205.\n",
            " Top K precision = 0.08720930232558136, recall = 0.007174715824248701.\n",
            "Training on epoch 2 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.741162, and regularization loss is 0.034939.\n",
            " Top K precision = 0.09456521739130432, recall = 0.008733971093709374.\n",
            "Training on epoch 2 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.734451, and regularization loss is 0.033577.\n",
            " Top K precision = 0.08202247191011235, recall = 0.008353664362566333.\n",
            "Training on epoch 2 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.791259, and regularization loss is 0.034833.\n",
            " Top K precision = 0.10396039603960393, recall = 0.008333990819554788.\n",
            "Training on epoch 2 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.884588, and regularization loss is 0.030683.\n",
            " Top K precision = 0.07954545454545449, recall = 0.007603167353540521.\n",
            "Training on epoch 2 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.753231, and regularization loss is 0.030706.\n",
            " Top K precision = 0.0863157894736842, recall = 0.006530132567365618.\n",
            "Training on epoch 2 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.725981, and regularization loss is 0.027701.\n",
            " Top K precision = 0.08387096774193545, recall = 0.006954276714292948.\n",
            "\n",
            "Training on 2 epoch completed.\n",
            " Average bpr_loss on train set is 0.00607 for the current epoch.\n",
            " Training top K precision = 0.0779999999999999, recall = 0.00634640927129663.\n",
            " Average bpr_loss on the validation set is 4.6e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07849999999999992, recall = 0.007076917372232163.\n",
            "\n",
            "Training on the 3 epoch\n",
            "Training on epoch 3 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.777526, and regularization loss is 0.030201.\n",
            " Top K precision = 0.0717391304347826, recall = 0.0063921943472841575.\n",
            "Training on epoch 3 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.733763, and regularization loss is 0.02737.\n",
            " Top K precision = 0.09183673469387751, recall = 0.005876988526056752.\n",
            "Training on epoch 3 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.791602, and regularization loss is 0.028711.\n",
            " Top K precision = 0.10212765957446801, recall = 0.008851857839679199.\n",
            "Training on epoch 3 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.736339, and regularization loss is 0.029573.\n",
            " Top K precision = 0.09999999999999999, recall = 0.007585966885750439.\n",
            "Training on epoch 3 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.792305, and regularization loss is 0.028756.\n",
            " Top K precision = 0.08988764044943819, recall = 0.008154490541174376.\n",
            "Training on epoch 3 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.742315, and regularization loss is 0.027395.\n",
            " Top K precision = 0.0835164835164835, recall = 0.0076796900996376274.\n",
            "Training on epoch 3 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.73369, and regularization loss is 0.025509.\n",
            " Top K precision = 0.09591836734693872, recall = 0.007390201217050378.\n",
            "Training on epoch 3 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.75861, and regularization loss is 0.026329.\n",
            " Top K precision = 0.08437499999999999, recall = 0.00855060593237955.\n",
            "\n",
            "Training on 3 epoch completed.\n",
            " Average bpr_loss on train set is 0.006015 for the current epoch.\n",
            " Training top K precision = 0.07249999999999995, recall = 0.005588440476931404.\n",
            " Average bpr_loss on the validation set is 4.8e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07799999999999992, recall = 0.007004453604116222.\n",
            "\n",
            "Training on the 4 epoch\n",
            "Training on epoch 4 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.806191, and regularization loss is 0.024726.\n",
            " Top K precision = 0.09361702127659571, recall = 0.008148230412564872.\n",
            "Training on epoch 4 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.731746, and regularization loss is 0.024781.\n",
            " Top K precision = 0.09892473118279567, recall = 0.009264924394856211.\n",
            "Training on epoch 4 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.721751, and regularization loss is 0.026506.\n",
            " Top K precision = 0.0857142857142857, recall = 0.008246420970269038.\n",
            "Training on epoch 4 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.731299, and regularization loss is 0.025025.\n",
            " Top K precision = 0.1010752688172043, recall = 0.008039816685874139.\n",
            "Training on epoch 4 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.718751, and regularization loss is 0.025155.\n",
            " Top K precision = 0.09895833333333331, recall = 0.00827079161027413.\n",
            "Training on epoch 4 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.748404, and regularization loss is 0.025727.\n",
            " Top K precision = 0.08265306122448979, recall = 0.006864588732383426.\n",
            "Training on epoch 4 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.722576, and regularization loss is 0.022065.\n",
            " Top K precision = 0.09062499999999996, recall = 0.007861965845068411.\n",
            "Training on epoch 4 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.718272, and regularization loss is 0.023308.\n",
            " Top K precision = 0.07849462365591396, recall = 0.007056571590767471.\n",
            "\n",
            "Training on 4 epoch completed.\n",
            " Average bpr_loss on train set is 0.006003 for the current epoch.\n",
            " Training top K precision = 0.0764999999999999, recall = 0.006431498966333944.\n",
            " Average bpr_loss on the validation set is 4.8e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07849999999999992, recall = 0.007076917372232163.\n",
            "\n",
            "Training on the 5 epoch\n",
            "Training on epoch 5 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.783479, and regularization loss is 0.024557.\n",
            " Top K precision = 0.08260869565217387, recall = 0.009422258806592965.\n",
            "Training on epoch 5 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.761851, and regularization loss is 0.024221.\n",
            " Top K precision = 0.0989690721649484, recall = 0.008013100194194225.\n",
            "Training on epoch 5 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.738245, and regularization loss is 0.022177.\n",
            " Top K precision = 0.07959183673469386, recall = 0.0071860569852373315.\n",
            "Training on epoch 5 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.731225, and regularization loss is 0.022437.\n",
            " Top K precision = 0.09479166666666662, recall = 0.007570197774375863.\n",
            "Training on epoch 5 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.722703, and regularization loss is 0.022436.\n",
            " Top K precision = 0.08631578947368418, recall = 0.00823399171435163.\n",
            "Training on epoch 5 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.717723, and regularization loss is 0.022154.\n",
            " Top K precision = 0.08469387755102038, recall = 0.00800313835548235.\n",
            "Training on epoch 5 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.726133, and regularization loss is 0.02129.\n",
            " Top K precision = 0.07731958762886595, recall = 0.008208736426700917.\n",
            "Training on epoch 5 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.71632, and regularization loss is 0.023281.\n",
            " Top K precision = 0.09462365591397848, recall = 0.005328864983174089.\n",
            "\n",
            "Training on 5 epoch completed.\n",
            " Average bpr_loss on train set is 0.005939 for the current epoch.\n",
            " Training top K precision = 0.06749999999999994, recall = 0.005592554494180134.\n",
            " Average bpr_loss on the validation set is 4.6e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07799999999999992, recall = 0.007004453604116222.\n",
            "\n",
            "Training on the 6 epoch\n",
            "Training on epoch 6 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.725632, and regularization loss is 0.023618.\n",
            " Top K precision = 0.10526315789473681, recall = 0.008853809530031098.\n",
            "Training on epoch 6 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.718267, and regularization loss is 0.023.\n",
            " Top K precision = 0.08958333333333329, recall = 0.0075504832047562295.\n",
            "Training on epoch 6 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.71767, and regularization loss is 0.021504.\n",
            " Top K precision = 0.081, recall = 0.0077868137144333546.\n",
            "Training on epoch 6 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.719854, and regularization loss is 0.020743.\n",
            " Top K precision = 0.09252336448598127, recall = 0.007440545516554321.\n",
            "Training on epoch 6 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.727211, and regularization loss is 0.020807.\n",
            " Top K precision = 0.0783505154639175, recall = 0.00801172919428596.\n",
            "Training on epoch 6 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.780046, and regularization loss is 0.019493.\n",
            " Top K precision = 0.07717391304347823, recall = 0.0072665112715692955.\n",
            "Training on epoch 6 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.737685, and regularization loss is 0.019026.\n",
            " Top K precision = 0.09183673469387747, recall = 0.009096298509620609.\n",
            "Training on epoch 6 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.750837, and regularization loss is 0.02079.\n",
            " Top K precision = 0.08333333333333333, recall = 0.0065547552031562884.\n",
            "\n",
            "Training on 6 epoch completed.\n",
            " Average bpr_loss on train set is 0.005901 for the current epoch.\n",
            " Training top K precision = 0.07599999999999996, recall = 0.0059357590917895635.\n",
            " Average bpr_loss on the validation set is 4.4e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07799999999999992, recall = 0.007004453604116222.\n",
            "\n",
            "Training on the 7 epoch\n",
            "Training on epoch 7 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.729126, and regularization loss is 0.021458.\n",
            " Top K precision = 0.08777777777777777, recall = 0.005972803094490795.\n",
            "Training on epoch 7 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.726569, and regularization loss is 0.015933.\n",
            " Top K precision = 0.06666666666666667, recall = 0.00606060997925834.\n",
            "Training on epoch 7 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.727603, and regularization loss is 0.022106.\n",
            " Top K precision = 0.07812499999999997, recall = 0.006936950341827198.\n",
            "Training on epoch 7 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.722979, and regularization loss is 0.02075.\n",
            " Top K precision = 0.11182795698924726, recall = 0.009534835897605645.\n",
            "Training on epoch 7 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.73893, and regularization loss is 0.019818.\n",
            " Top K precision = 0.09148936170212761, recall = 0.006205603578161333.\n",
            "Training on epoch 7 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.773177, and regularization loss is 0.0193.\n",
            " Top K precision = 0.08469387755102036, recall = 0.007994786247198911.\n",
            "Training on epoch 7 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.744637, and regularization loss is 0.01907.\n",
            " Top K precision = 0.08947368421052627, recall = 0.0071983075506876016.\n",
            "Training on epoch 7 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.723638, and regularization loss is 0.019632.\n",
            " Top K precision = 0.09361702127659573, recall = 0.0068635898999311355.\n",
            "\n",
            "Training on 7 epoch completed.\n",
            " Average bpr_loss on train set is 0.005798 for the current epoch.\n",
            " Training top K precision = 0.07499999999999991, recall = 0.00685531641275682.\n",
            " Average bpr_loss on the validation set is 4e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07849999999999992, recall = 0.007076917372232163.\n",
            "\n",
            "Training on the 8 epoch\n",
            "Training on epoch 8 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.71431, and regularization loss is 0.018133.\n",
            " Top K precision = 0.09052631578947362, recall = 0.007499234046519785.\n",
            "Training on epoch 8 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.725472, and regularization loss is 0.021417.\n",
            " Top K precision = 0.10212765957446804, recall = 0.009006984081567538.\n",
            "Training on epoch 8 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.966709, and regularization loss is 0.018856.\n",
            " Top K precision = 0.098876404494382, recall = 0.008169497753187047.\n",
            "Training on epoch 8 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.726764, and regularization loss is 0.019949.\n",
            " Top K precision = 0.10104166666666663, recall = 0.008141234463995693.\n",
            "Training on epoch 8 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.734458, and regularization loss is 0.018166.\n",
            " Top K precision = 0.09673913043478258, recall = 0.006712091548280995.\n",
            "Training on epoch 8 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.716648, and regularization loss is 0.018587.\n",
            " Top K precision = 0.08602150537634408, recall = 0.007830171454563712.\n",
            "Training on epoch 8 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.717458, and regularization loss is 0.018337.\n",
            " Top K precision = 0.07978723404255317, recall = 0.006839446425374466.\n",
            "Training on epoch 8 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.732379, and regularization loss is 0.016184.\n",
            " Top K precision = 0.0791666666666666, recall = 0.007851774459925536.\n",
            "\n",
            "Training on 8 epoch completed.\n",
            " Average bpr_loss on train set is 0.006053 for the current epoch.\n",
            " Training top K precision = 0.07449999999999993, recall = 0.0057737660517361645.\n",
            " Average bpr_loss on the validation set is 4.8e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07849999999999992, recall = 0.007076917372232163.\n",
            "\n",
            "Training on the 9 epoch\n",
            "Training on epoch 9 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.730109, and regularization loss is 0.018223.\n",
            " Top K precision = 0.08315789473684206, recall = 0.006190760875873594.\n",
            "Training on epoch 9 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.712369, and regularization loss is 0.017475.\n",
            " Top K precision = 0.08762886597938141, recall = 0.006340923959067784.\n",
            "Training on epoch 9 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.717283, and regularization loss is 0.018948.\n",
            " Top K precision = 0.10103092783505149, recall = 0.009901384070334236.\n",
            "Training on epoch 9 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.716512, and regularization loss is 0.016492.\n",
            " Top K precision = 0.08877551020408159, recall = 0.008172525536554107.\n",
            "Training on epoch 9 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.750307, and regularization loss is 0.020616.\n",
            " Top K precision = 0.10106382978723401, recall = 0.006761265506472768.\n",
            "Training on epoch 9 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.723181, and regularization loss is 0.015494.\n",
            " Top K precision = 0.07857142857142854, recall = 0.0067182519705738485.\n",
            "Training on epoch 9 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.756514, and regularization loss is 0.019058.\n",
            " Top K precision = 0.09042553191489358, recall = 0.00857163630276696.\n",
            "Training on epoch 9 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.737944, and regularization loss is 0.017071.\n",
            " Top K precision = 0.06236559139784944, recall = 0.004695208806916296.\n",
            "\n",
            "Training on 9 epoch completed.\n",
            " Average bpr_loss on train set is 0.005827 for the current epoch.\n",
            " Training top K precision = 0.07149999999999988, recall = 0.006500562271522456.\n",
            " Average bpr_loss on the validation set is 4.4e-05, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.07849999999999992, recall = 0.007076917372232163.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing the different models "
      ],
      "metadata": {
        "id": "FLDOkEO7dpH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising the precision values"
      ],
      "metadata": {
        "id": "cz0b-A3Xdv5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = plt.figure()\n",
        "f.set_figwidth(10)\n",
        "f.set_figheight(10)\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in train_topks],\n",
        "         label=\"Train1\")\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in val_topks],\n",
        "         label=\"Val1\")\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in train_topks2],\n",
        "         label=\"Train2\")\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in val_topks2],\n",
        "         label=\"Val2\")\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in train_topks3],\n",
        "         label=\"Train3\")\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in val_topks3],\n",
        "         label=\"Val3\")\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in train_topks4],\n",
        "         label=\"Train4\")\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in val_topks4],\n",
        "         label=\"Val4\")\n",
        "plt.ylabel(f\"Top {K} precision\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "Ydddvq0VCHGj",
        "outputId": "af31188b-ddff-42dd-b4e1-924ef56524ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising the recall values"
      ],
      "metadata": {
        "id": "PtRCtWo1d2k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = plt.figure()\n",
        "f.set_figwidth(10)\n",
        "f.set_figheight(10)\n",
        "plt.plot(epochs_tracked, [recall for _, recall in train_topks],\n",
        "         label=\"Train1\")\n",
        "plt.plot(epochs_tracked, [recall for _, recall in val_topks],\n",
        "         label=\"Val1\")\n",
        "plt.plot(epochs_tracked, [recall for _, recall in train_topks2],\n",
        "         label=\"Train2\")\n",
        "plt.plot(epochs_tracked, [recall for _, recall in val_topks2],\n",
        "         label=\"Val2\")\n",
        "plt.plot(epochs_tracked, [recall for _, recall in train_topks3],\n",
        "         label=\"Train3\")\n",
        "plt.plot(epochs_tracked, [recall for _, recall in val_topks3],\n",
        "         label=\"Val3\")\n",
        "plt.plot(epochs_tracked, [recall for _, recall in train_topks4],\n",
        "         label=\"Train4\")\n",
        "plt.plot(epochs_tracked, [recall for _, recall in val_topks4],\n",
        "         label=\"Val4\")\n",
        "plt.ylabel(f\"Top {K} recall\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "Dv8G7o7vEWcI",
        "outputId": "26764dfd-fa2e-45c0-cd03-0b85fc02ba06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction using the model3"
      ],
      "metadata": {
        "id": "0VKQ8IyTeAF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technique using the item counts and the user counts is used to predict the values on the test set"
      ],
      "metadata": {
        "id": "-fX1sUMjeFsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on the test set\n",
        "lightGCN3.eval()\n",
        "print(\"Training completed after {} epochs\".format(epochs))\n",
        "\n",
        "users_test = samples_test[:, 0:1]\n",
        "pos_test = samples_test[:, 1:2]\n",
        "neg_test = samples_test[:, 2:3]\n",
        "\n",
        "loss_test, reg_loss_test = bpr_loss(\n",
        "    lightGCN3, users_test, pos_test, neg_test, data, test_mask)\n",
        "reg_loss_test = reg_loss_test * weight_decay\n",
        "\n",
        "# predict on the test set\n",
        "user_indices = samples_test[:, 0]\n",
        "user_indices = user_indices.repeat(2).long()\n",
        "item_indices = torch.cat((samples_test[:, 1], samples_test[:, 2])).long()\n",
        "pred_test = getUsersRating(lightGCN3, users_test[:,0], data)\\\n",
        "    [user_indices, item_indices]\n",
        "truth_test = data[\"edge_index\"][users_test.long()[:,0]]\\\n",
        "    [user_indices, item_indices]\n",
        "test_topk_precision, test_topk_recall = personalized_topk(\n",
        "    pred_test, K, user_indices, data[\"edge_index\"])\n",
        "\n",
        "print(\"Average bpr_loss on the test set is {}, and regularization loss is {}.\\n\".format(round(float((loss_test+reg_loss_test)/len(samples_test)), 6),\n",
        "                                                                                                round(float(reg_loss_test/len(samples_test)), 6)),\n",
        "      \"Top K precision = {}, recall = {}.\".format(test_topk_precision, test_topk_recall))\n",
        "\n",
        "# Save model embeddings.\n",
        "torch.save(lightGCN3, config_dict[\"model_name\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJaBAe4gEY-U",
        "outputId": "ed21a9bc-5f14-4034-e7e5-bfde7ea5646a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed after 10 epochs\n",
            "Average bpr_loss on the test set is 7e-06, and regularization loss is 0.0.\n",
            " Top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing LightGCN with baseline MF technique"
      ],
      "metadata": {
        "id": "ZoIf3xX6eM9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_factorization(user_item, rank):\n",
        "    \"\"\"Runs matrix factorization on `user_item` and get user-item similarities.\n",
        "\n",
        "    Args:\n",
        "        user_item: User-item connectivity matrix.\n",
        "        rank: Number of numbers to represent a user / item.\n",
        "\n",
        "    Returns:\n",
        "        User-item similarities.\n",
        "    \"\"\"\n",
        "    weights, (user_factors, item_factors) = \\\n",
        "        decomposition.parafac(user_item, rank)\n",
        "    similarities = user_factors @ item_factors.T\n",
        "    return 1 / (1 + np.exp(- similarities))"
      ],
      "metadata": {
        "id": "WIzuXo2HEb6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute baseline metrics using matrix factorization.\n",
        "baseline_pred = matrix_factorization(\n",
        "        data[\"edge_index\"].detach().cpu().numpy(),\n",
        "        config_dict[\"mf_rank\"])[user_indices.cpu(), item_indices.cpu()]\n",
        "baseline_topk_precision, baseline_topk_recall = \\\n",
        "        personalized_topk(baseline_pred, K, user_indices, data[\"edge_index\"])\n",
        "print(\"Baseline (PARAFAC matrix factorization) produces \",\n",
        "      \"Top K precision = {}, recall = {}.\".format(baseline_topk_precision,\n",
        "                                                  baseline_topk_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOxGlrcvEh3X",
        "outputId": "035a274d-053e-4cc0-efb6-5d7a5fe2f81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline (PARAFAC matrix factorization) produces  Top K precision = 0.032499999999999994, recall = 0.0025960504440079523.\n"
          ]
        }
      ]
    }
  ]
}